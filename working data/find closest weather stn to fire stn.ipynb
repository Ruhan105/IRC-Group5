{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get closest weather stations for each fire station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fire_id                              fire_name closest_weather_id  distance\n",
      "0      AEU            Amador - El Dorado CAL FIRE        US1CAAM0003  0.044820\n",
      "1      ANF                Angeles National Forest        USR0000CCHI  0.069613\n",
      "2      BRR  Bitter Creek National Wildlife Refuge        USC00046754  0.023635\n",
      "3      BTU                         Butte CAL FIRE        USC00046685  0.000916\n",
      "4      MCP       Camp Pendleton Marine Corps Base        USW00000369  0.027352\n",
      "..     ...                                    ...                ...       ...\n",
      "78     TCU          Tuolumne - Calaveras CAL FIRE        USC00046172  0.083267\n",
      "79     VLJ                     Vallejo Fire Dept.        USC00045333  0.045435\n",
      "80     AFV              Vandenberg Air Force Base        USW00093214  0.026880\n",
      "81     VNC                         Ventura County        US1CAVT0031  0.067191\n",
      "82     YNP                 Yosemite National Park        USC00049855  0.115259\n",
      "\n",
      "[83 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "fire_stations = pd.read_csv(\"all fire stations in area_ fire.csv\")\n",
    "weather_stations = pd.read_csv(\"ghcnd-stations.csv\")\n",
    "\n",
    "def dist(x, y):\n",
    "    \"\"\"Calculate Euclidean distance between two coordinate points.\"\"\"\n",
    "    return np.sqrt((x[0] - y[0])**2 + (x[1] - y[1])**2)\n",
    "\n",
    "def find_closest(fire_stations: pd.DataFrame, weather_stations: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the closest weather station to each fire station.\n",
    "    \n",
    "    Parameters:\n",
    "        fire_stations (pd.DataFrame): DataFrame with columns ['fire_id', 'fire_name', 'lat', 'lon']\n",
    "        weather_stations (pd.DataFrame): DataFrame with columns ['weather_id', 'lat', 'lon']\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with ['fire_id', 'fire_name', 'closest_weather_id', 'distance']\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, fire in fire_stations.iterrows():\n",
    "        fire_coords = (fire[\"lat\"], fire[\"lon\"])\n",
    "        min_dist = float(\"inf\")\n",
    "        closest_weather = None\n",
    "\n",
    "        for _, weather in weather_stations.iterrows():\n",
    "            weather_coords = (weather[\"lat\"], weather[\"lon\"])\n",
    "            distance = dist(fire_coords, weather_coords)\n",
    "\n",
    "            if distance < min_dist:\n",
    "                min_dist = distance\n",
    "                closest_weather = weather[\"weather_id\"]\n",
    "\n",
    "        # ✅ Fix: Correct column access using `.loc`\n",
    "        results.append([fire[\"fire_id\"], fire[\"fire_name\"], closest_weather, min_dist])\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    closest_df = pd.DataFrame(results, columns=[\"fire_id\", \"fire_name\", \"closest_weather_id\", \"distance\"])\n",
    "    return closest_df\n",
    "\n",
    "# Run function\n",
    "closest_df = find_closest(fire_stations, weather_stations)\n",
    "print(closest_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireweather_conv=closest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch weather data from NOAA ftp with the list of station id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherstn_list = closest_df['closest_weather_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "\n",
    "\n",
    "\n",
    "# Directory to save downloaded files\n",
    "download_dir = \"weather_ftpfetched\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "for station_id in weatherstn_list:\n",
    "    file_name = f\"{station_id}.csv.gz\"  # NOAA files are in .csv.gz format\n",
    "    file_url = base_url + file_name\n",
    "    local_file_path = os.path.join(download_dir, file_name)\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(file_url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(local_file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {file_name} (Status Code: {response.status_code})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert csv.gz to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Define the source folder containing .csv.gz files\n",
    "data_folder = \"weather_ftpfetched\"\n",
    "\n",
    "# Define the destination folder for converted .csv files\n",
    "converted_folder = \"weathercsv_converted\"\n",
    "\n",
    "# Create the converted folder if it doesn't exist\n",
    "os.makedirs(converted_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all .csv.gz files in the data folder\n",
    "all_files = [f for f in os.listdir(data_folder) if f.endswith('.csv.gz')]\n",
    "\n",
    "# Process each .csv.gz file\n",
    "for file in all_files:\n",
    "    input_path = os.path.join(data_folder, file)  # Full path to input file\n",
    "    output_filename = file.replace(\".csv.gz\", \".csv\")  # Change file extension\n",
    "    output_path = os.path.join(converted_folder, output_filename)  # Full path to output file\n",
    "\n",
    "    # Open the .gz file and read it using pandas\n",
    "    with gzip.open(input_path, 'rt', encoding='utf-8') as f:  # Read in text mode\n",
    "        try:\n",
    "            # Read the CSV file, skipping bad lines\n",
    "            df = pd.read_csv(f, low_memory=False, on_bad_lines='skip', sep=',')\n",
    "\n",
    "            # Print row count for debugging\n",
    "            print(f\"✅ Read {file} with {len(df)} rows.\")\n",
    "\n",
    "            # Save the converted .csv file\n",
    "            df.to_csv(output_path, index=False)\n",
    "\n",
    "            print(f\"📁 Saved converted file to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next step: combine all the csv into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed and saved US1CAAM0003.csv\n",
      "Fixed and saved US1CACL0001.csv\n",
      "Fixed and saved US1CADN0012.csv\n",
      "Fixed and saved US1CAFR0033.csv\n",
      "Fixed and saved US1CAHM0029.csv\n",
      "Fixed and saved US1CAHM0144.csv\n",
      "Fixed and saved US1CALK0018.csv\n",
      "Fixed and saved US1CAMD0033.csv\n",
      "Fixed and saved US1CAMR0002.csv\n",
      "Fixed and saved US1CAMR0011.csv\n",
      "Fixed and saved US1CASC0006.csv\n",
      "Fixed and saved US1CASD0026.csv\n",
      "Fixed and saved US1CASK0016.csv\n",
      "Fixed and saved US1CASL0040.csv\n",
      "Fixed and saved US1CASU0005.csv\n",
      "Fixed and saved US1CASZ0043.csv\n",
      "Fixed and saved US1CAVT0017.csv\n",
      "Fixed and saved US1CAVT0031.csv\n",
      "Fixed and saved USC00040134.csv\n",
      "Fixed and saved USC00040161.csv\n",
      "Fixed and saved USC00040204.csv\n",
      "Fixed and saved USC00040332.csv\n",
      "Fixed and saved USC00040543.csv\n",
      "Fixed and saved USC00040798.csv\n",
      "Fixed and saved USC00041018.csv\n",
      "Fixed and saved USC00041075.csv\n",
      "Fixed and saved USC00041784.csv\n",
      "Fixed and saved USC00041799.csv\n",
      "Fixed and saved USC00041805.csv\n",
      "Fixed and saved USC00041906.csv\n",
      "Fixed and saved USC00042027.csv\n",
      "Fixed and saved USC00042269.csv\n",
      "Fixed and saved USC00042580.csv\n",
      "Fixed and saved USC00044176.csv\n",
      "Fixed and saved USC00044518.csv\n",
      "Fixed and saved USC00044616.csv\n",
      "Fixed and saved USC00045151.csv\n",
      "Fixed and saved USC00045212.csv\n",
      "Fixed and saved USC00045311.csv\n",
      "Fixed and saved USC00045333.csv\n",
      "Fixed and saved USC00045352.csv\n",
      "Fixed and saved USC00046136.csv\n",
      "Fixed and saved USC00046144.csv\n",
      "Fixed and saved USC00046172.csv\n",
      "Fixed and saved USC00046642.csv\n",
      "Fixed and saved USC00046685.csv\n",
      "Fixed and saved USC00046754.csv\n",
      "Fixed and saved USC00047085.csv\n",
      "Fixed and saved USC00047775.csv\n",
      "Fixed and saved USC00047950.csv\n",
      "Fixed and saved USC00048105.csv\n",
      "Fixed and saved USC00048135.csv\n",
      "Fixed and saved USC00048338.csv\n",
      "Fixed and saved USC00048702.csv\n",
      "Fixed and saved USC00049053.csv\n",
      "Fixed and saved USC00049083.csv\n",
      "Fixed and saved USC00049102.csv\n",
      "Fixed and saved USC00049600.csv\n",
      "Fixed and saved USC00049855.csv\n",
      "Fixed and saved USR0000CALD.csv\n",
      "Fixed and saved USR0000CBIH.csv\n",
      "Fixed and saved USR0000CCHI.csv\n",
      "Fixed and saved USR0000CCRE.csv\n",
      "Fixed and saved USR0000CFIG.csv\n",
      "Fixed and saved USR0000CIND.csv\n",
      "Fixed and saved USR0000CLKL.csv\n",
      "Fixed and saved USR0000CMIO.csv\n",
      "Fixed and saved USR0000CPIR.csv\n",
      "Fixed and saved USR0000CSAC.csv\n",
      "Fixed and saved USR0000CSMI.csv\n",
      "Fixed and saved USR0000CSNL.csv\n",
      "Fixed and saved USR0000CTIM.csv\n",
      "Fixed and saved USR0000CWEE.csv\n",
      "Fixed and saved USS0019L03S.csv\n",
      "Fixed and saved USW00000369.csv\n",
      "Fixed and saved USW00023248.csv\n",
      "Fixed and saved USW00053139.csv\n",
      "Fixed and saved USW00093115.csv\n",
      "Fixed and saved USW00093118.csv\n",
      "Fixed and saved USW00093205.csv\n",
      "Fixed and saved USW00093214.csv\n",
      "Fixed and saved USW00093243.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder where your CSV files are stored\n",
    "data_folder = \"weathercsv_converted\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Define the expected columns\n",
    "expected_columns = ['id', 'date', 'obs', 'obs_value']\n",
    "\n",
    "# Iterate through all CSV files to ensure they have the same structure\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    \n",
    "    try:\n",
    "        # Read the current CSV file\n",
    "        df = pd.read_csv(file_path,low_memory=False)\n",
    "        \n",
    "        # Drop columns with 'Unnamed' in the name (extra columns)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        \n",
    "        # Check if the number of columns matches the expected structure\n",
    "        if len(df.columns) >= 4:\n",
    "            # Ensure the first four columns are the expected ones\n",
    "            df = df.iloc[:, :4]  # Select the first 4 columns\n",
    "            df.columns = expected_columns  # Rename the columns\n",
    "\n",
    "            # Save the fixed CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Fixed and saved {file}\")\n",
    "        else:\n",
    "            print(f\"Skipping {file}: Not enough columns to modify.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# After this, all CSVs in the folder should have the same structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine all the weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder where your CSV files are stored\n",
    "data_folder = \"weathercsv_converted\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Define the expected columns\n",
    "expected_columns = ['id', 'date', 'obs', 'obs_value']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through all CSV files and read them\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Drop unnamed columns (extra columns)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed', na=False)]\n",
    "        \n",
    "        # Ensure it has at least 4 columns\n",
    "        if len(df.columns) >= 4:\n",
    "            df = df.iloc[:, :4]  # Keep only the first four columns\n",
    "            df.columns = expected_columns  # Rename the columns\n",
    "            \n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "            print(f\"Added {file} to combined dataset.\")\n",
    "        else:\n",
    "            print(f\"Skipping {file}: Not enough columns.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(\"All CSV files successfully combined into a single DataFrame.\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame(columns=expected_columns)  # Return an empty DataFrame if no valid files\n",
    "    print(\"No valid CSV files found to combine.\")\n",
    "\n",
    "# Display the DataFrame (optional)\n",
    "print(combined_df.head())  # Show the first few rows\n",
    "\n",
    "# The variable `combined_df` now holds the full dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pivot weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_pivoted= combined_df.pivot(values=\"obs_value\",index=[\"id\",\"date\"],columns=\"obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_data = pd.read_csv(r'fire_data.csv')\n",
    "fire_data = fire_data.dropna()\n",
    "\n",
    "# Drop columns if they exist\n",
    "columns_to_drop = ['CONT_DATE', '_id']\n",
    "fire_data = fire_data.drop(columns=[col for col in columns_to_drop if col in fire_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add corresponding weather station to fire data\n",
    "\n",
    "fire_data[\"weatherstn\"] = fire_data[\"Fire station name\"].map(closest_df.set_index(\"fire_name\")[\"closest_weather_id\"])\n",
    "fire_data= fire_data.set_index('weatherstn')\n",
    "\n",
    "allfirestn= fire_data['Fire station name'].unique().tolist()\n",
    "fire_data['Fire station name'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing IDs: ['Carson City District - BLM', 'Bakersfield District - BLM (retired code)', 'City of Weed Vol. Fire Dept.', 'San Diego CAL FIRE (retired code)', 'Central CA District - BLM', 'Fort Yuma Agency', 'Beale Air Force Base FD', 'Golden Gate National Recreation Area - NPS', 'Sequoia - Kings Canyon NP', 'Fremont National Forest', 'Siskiyou National Forest', 'Colorado River District', 'Northern CA District - BLM', 'Colorado River Agency', 'Lakeview District', 'CA Desert District - BLM', 'Orange County', 'Mojave - NPS', 'San Diego CAL FIRE', 'Toiyabe National Forest']\n"
     ]
    }
   ],
   "source": [
    "#we have 103 stations, but there is only 83 station in the fire station list\n",
    "#have to redo the list and run all the code again\n",
    "#good news: only have to change the fire station data\n",
    "\n",
    "#first, find out what is missing in the 103, so we can simply add them in \n",
    "\n",
    "missing_stn = list(set(allfirestn) - set(fire_stations[\"fire_name\"]))\n",
    "print(\"Missing IDs:\", missing_stn)\n",
    "\n",
    "#OHH after checking: the missing station does not belong to California\n",
    "#will simpy drop them from the fire dataset\n",
    "\n",
    "\n",
    "# Drop rows where 'id' is in criteria_list\n",
    "df_filtered = fire_data[~fire_data['Fire station name'].isin(missing_stn)]\n",
    "\n",
    "fire_data=df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping data, filling in missing with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#weather data reshaped: weather_pivoted\n",
    "#weather unpivoted (apparently its better to leave data unpivoted for ML?):combined_df\n",
    "#fire data with weather station id: fire_data\n",
    "\n",
    "weather_dataall= weather_pivoted.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try merging with pivoted but separated index weather data\n",
    "# Convert both 'date' columns to datetime with the correct format\n",
    "weather_dataall['date'] = pd.to_datetime(weather_dataall['date'], format='%d/%m/%Y')\n",
    "fire_data['ALARM_DATE'] = pd.to_datetime(fire_data['ALARM_DATE'], format='%d/%m/%Y')\n",
    "\n",
    "weather_fire_pivotmerge = pd.merge(weather_dataall, fire_data, left_on=['id', 'date'], right_on=['weatherstn', 'ALARM_DATE'], how='left')\n",
    "# Add a 'fire_occurred' column where 1 indicates fire occurred (fire_area is not NaN) and 0 indicates no fire\n",
    "weather_fire_pivotmerge['fire_occurred'] = weather_fire_pivotmerge['Shape__Area'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try merging with unpivoted index weather data, tall and skinny data\n",
    "unpivot_weather =combined_df\n",
    "# Convert both 'date' columns to datetime with the correct format\n",
    "unpivot_weather['date'] = pd.to_datetime(unpivot_weather['date'], format='%Y%m%d')\n",
    "fire_data['ALARM_DATE'] = pd.to_datetime(fire_data['ALARM_DATE'], format='%Y%m%d')\n",
    "\n",
    "unpivotweather_fire_merge = pd.merge(unpivot_weather, fire_data, left_on=['id', 'date'], right_on=['weatherstn', 'ALARM_DATE'], how='left')\n",
    "# Add a 'fire_occurred' column where 1 indicates fire occurred (fire_area is not NaN) and 0 indicates no fire\n",
    "unpivotweather_fire_merge['fire_occurred'] = unpivotweather_fire_merge['Shape__Area'].notna().astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
