{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get closest weather stations for each fire station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closest_df\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Run function\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m closest_df \u001b[38;5;241m=\u001b[39m find_closest(fire_stations, weather_stations)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(closest_df)\n",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m, in \u001b[0;36mfind_closest\u001b[1;34m(fire_stations, weather_stations)\u001b[0m\n\u001b[0;32m     27\u001b[0m min_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m closest_weather \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, weather \u001b[38;5;129;01min\u001b[39;00m weather_stations\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     31\u001b[0m     weather_coords \u001b[38;5;241m=\u001b[39m (weather[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m], weather[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     32\u001b[0m     distance \u001b[38;5;241m=\u001b[39m dist(fire_coords, weather_coords)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m klass(v, index\u001b[38;5;241m=\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mk)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[0;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:593\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    590\u001b[0m         data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n\u001b[0;32m    592\u001b[0m NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m--> 593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_axis(\u001b[38;5;241m0\u001b[39m, index)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6320\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6317\u001b[0m \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[0;32m   6318\u001b[0m \u001b[38;5;66;03m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[0;32m   6319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set:\n\u001b[1;32m-> 6320\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m   6321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata:\n\u001b[0;32m   6322\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:786\u001b[0m, in \u001b[0;36mSeries.name\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;129m@name\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 786\u001b[0m     validate_all_hashable(value, error_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, value)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1590\u001b[0m, in \u001b[0;36mvalidate_all_hashable\u001b[1;34m(error_name, *args)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_all_hashable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, error_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;124;03m    Return None if all args are hashable, else raise a TypeError.\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_hashable(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[0;32m   1591\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m error_name:\n\u001b[0;32m   1592\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be a hashable type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1590\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_all_hashable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, error_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;124;03m    Return None if all args are hashable, else raise a TypeError.\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_hashable(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[0;32m   1591\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m error_name:\n\u001b[0;32m   1592\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be a hashable type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "fire_stations = pd.read_csv(\"all fire stations in area_ fire.csv\")\n",
    "weather_stations = pd.read_csv(\"ghcnd-stations.csv\")\n",
    "\n",
    "def dist(x, y):\n",
    "    \"\"\"Calculate Euclidean distance between two coordinate points.\"\"\"\n",
    "    return np.sqrt((x[0] - y[0])**2 + (x[1] - y[1])**2)\n",
    "\n",
    "def find_closest(fire_stations: pd.DataFrame, weather_stations: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the closest weather station to each fire station.\n",
    "    \n",
    "    Parameters:\n",
    "        fire_stations (pd.DataFrame): DataFrame with columns ['fire_id', 'fire_name', 'lat', 'lon']\n",
    "        weather_stations (pd.DataFrame): DataFrame with columns ['weather_id', 'lat', 'lon']\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with ['fire_id', 'fire_name', 'closest_weather_id', 'distance']\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, fire in fire_stations.iterrows():\n",
    "        fire_coords = (fire[\"lat\"], fire[\"lon\"])\n",
    "        min_dist = float(\"inf\")\n",
    "        closest_weather = None\n",
    "\n",
    "        for _, weather in weather_stations.iterrows():\n",
    "            weather_coords = (weather[\"lat\"], weather[\"lon\"])\n",
    "            distance = dist(fire_coords, weather_coords)\n",
    "\n",
    "            if distance < min_dist:\n",
    "                min_dist = distance\n",
    "                closest_weather = weather[\"weather_id\"]\n",
    "\n",
    "        # ✅ Fix: Correct column access using `.loc`\n",
    "        results.append([fire[\"fire_id\"], fire[\"fire_name\"], closest_weather, min_dist])\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    closest_df = pd.DataFrame(results, columns=[\"fire_id\", \"fire_name\", \"closest_weather_id\", \"distance\"])\n",
    "    return closest_df\n",
    "\n",
    "# Run function\n",
    "closest_df = find_closest(fire_stations, weather_stations)\n",
    "print(closest_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireweather_conv=closest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch weather data from NOAA ftp with the list of station id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherstn_list = closest_df['closest_weather_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "\n",
    "\n",
    "\n",
    "# Directory to save downloaded files\n",
    "download_dir = \"weather_ftpfetched\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "for station_id in weatherstn_list:\n",
    "    file_name = f\"{station_id}.csv.gz\"  # NOAA files are in .csv.gz format\n",
    "    file_url = base_url + file_name\n",
    "    local_file_path = os.path.join(download_dir, file_name)\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(file_url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(local_file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {file_name} (Status Code: {response.status_code})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert csv.gz to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Define the source folder containing .csv.gz files\n",
    "data_folder = \"weather_ftpfetched\"\n",
    "\n",
    "# Define the destination folder for converted .csv files\n",
    "converted_folder = \"weathercsv_converted\"\n",
    "\n",
    "# Create the converted folder if it doesn't exist\n",
    "os.makedirs(converted_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all .csv.gz files in the data folder\n",
    "all_files = [f for f in os.listdir(data_folder) if f.endswith('.csv.gz')]\n",
    "\n",
    "# Process each .csv.gz file\n",
    "for file in all_files:\n",
    "    input_path = os.path.join(data_folder, file)  # Full path to input file\n",
    "    output_filename = file.replace(\".csv.gz\", \".csv\")  # Change file extension\n",
    "    output_path = os.path.join(converted_folder, output_filename)  # Full path to output file\n",
    "\n",
    "    # Open the .gz file and read it using pandas\n",
    "    with gzip.open(input_path, 'rt', encoding='utf-8') as f:  # Read in text mode\n",
    "        try:\n",
    "            # Read the CSV file, skipping bad lines\n",
    "            df = pd.read_csv(f, low_memory=False, on_bad_lines='skip', sep=',')\n",
    "\n",
    "            # Print row count for debugging\n",
    "            print(f\"✅ Read {file} with {len(df)} rows.\")\n",
    "\n",
    "            # Save the converted .csv file\n",
    "            df.to_csv(output_path, index=False)\n",
    "\n",
    "            print(f\"📁 Saved converted file to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next step: combine all the csv into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed and saved US1CAAM0003.csv\n",
      "Fixed and saved US1CACL0001.csv\n",
      "Fixed and saved US1CADN0012.csv\n",
      "Fixed and saved US1CAFR0033.csv\n",
      "Fixed and saved US1CAHM0029.csv\n",
      "Fixed and saved US1CAHM0144.csv\n",
      "Fixed and saved US1CALK0018.csv\n",
      "Fixed and saved US1CAMD0033.csv\n",
      "Fixed and saved US1CAMR0002.csv\n",
      "Fixed and saved US1CAMR0011.csv\n",
      "Fixed and saved US1CASC0006.csv\n",
      "Fixed and saved US1CASD0026.csv\n",
      "Fixed and saved US1CASK0016.csv\n",
      "Fixed and saved US1CASL0040.csv\n",
      "Fixed and saved US1CASU0005.csv\n",
      "Fixed and saved US1CASZ0043.csv\n",
      "Fixed and saved US1CAVT0017.csv\n",
      "Fixed and saved US1CAVT0031.csv\n",
      "Fixed and saved USC00040134.csv\n",
      "Fixed and saved USC00040161.csv\n",
      "Fixed and saved USC00040204.csv\n",
      "Fixed and saved USC00040332.csv\n",
      "Fixed and saved USC00040543.csv\n",
      "Fixed and saved USC00040798.csv\n",
      "Fixed and saved USC00041018.csv\n",
      "Fixed and saved USC00041075.csv\n",
      "Fixed and saved USC00041784.csv\n",
      "Fixed and saved USC00041799.csv\n",
      "Fixed and saved USC00041805.csv\n",
      "Fixed and saved USC00041906.csv\n",
      "Fixed and saved USC00042027.csv\n",
      "Fixed and saved USC00042269.csv\n",
      "Fixed and saved USC00042580.csv\n",
      "Fixed and saved USC00044176.csv\n",
      "Fixed and saved USC00044518.csv\n",
      "Fixed and saved USC00044616.csv\n",
      "Fixed and saved USC00045151.csv\n",
      "Fixed and saved USC00045212.csv\n",
      "Fixed and saved USC00045311.csv\n",
      "Fixed and saved USC00045333.csv\n",
      "Fixed and saved USC00045352.csv\n",
      "Fixed and saved USC00046136.csv\n",
      "Fixed and saved USC00046144.csv\n",
      "Fixed and saved USC00046172.csv\n",
      "Fixed and saved USC00046642.csv\n",
      "Fixed and saved USC00046685.csv\n",
      "Fixed and saved USC00046754.csv\n",
      "Fixed and saved USC00047085.csv\n",
      "Fixed and saved USC00047775.csv\n",
      "Fixed and saved USC00047950.csv\n",
      "Fixed and saved USC00048105.csv\n",
      "Fixed and saved USC00048135.csv\n",
      "Fixed and saved USC00048338.csv\n",
      "Fixed and saved USC00048702.csv\n",
      "Fixed and saved USC00049053.csv\n",
      "Fixed and saved USC00049083.csv\n",
      "Fixed and saved USC00049102.csv\n",
      "Fixed and saved USC00049600.csv\n",
      "Fixed and saved USC00049855.csv\n",
      "Fixed and saved USR0000CALD.csv\n",
      "Fixed and saved USR0000CBIH.csv\n",
      "Fixed and saved USR0000CCHI.csv\n",
      "Fixed and saved USR0000CCRE.csv\n",
      "Fixed and saved USR0000CFIG.csv\n",
      "Fixed and saved USR0000CIND.csv\n",
      "Fixed and saved USR0000CLKL.csv\n",
      "Fixed and saved USR0000CMIO.csv\n",
      "Fixed and saved USR0000CPIR.csv\n",
      "Fixed and saved USR0000CSAC.csv\n",
      "Fixed and saved USR0000CSMI.csv\n",
      "Fixed and saved USR0000CSNL.csv\n",
      "Fixed and saved USR0000CTIM.csv\n",
      "Fixed and saved USR0000CWEE.csv\n",
      "Fixed and saved USS0019L03S.csv\n",
      "Fixed and saved USW00000369.csv\n",
      "Fixed and saved USW00023248.csv\n",
      "Fixed and saved USW00053139.csv\n",
      "Fixed and saved USW00093115.csv\n",
      "Fixed and saved USW00093118.csv\n",
      "Fixed and saved USW00093205.csv\n",
      "Fixed and saved USW00093214.csv\n",
      "Fixed and saved USW00093243.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder where your CSV files are stored\n",
    "data_folder = \"weathercsv_converted\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Define the expected columns\n",
    "expected_columns = ['id', 'date', 'obs', 'obs_value']\n",
    "\n",
    "# Iterate through all CSV files to ensure they have the same structure\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    \n",
    "    try:\n",
    "        # Read the current CSV file\n",
    "        df = pd.read_csv(file_path,low_memory=False)\n",
    "        \n",
    "        # Drop columns with 'Unnamed' in the name (extra columns)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        \n",
    "        # Check if the number of columns matches the expected structure\n",
    "        if len(df.columns) >= 4:\n",
    "            # Ensure the first four columns are the expected ones\n",
    "            df = df.iloc[:, :4]  # Select the first 4 columns\n",
    "            df.columns = expected_columns  # Rename the columns\n",
    "\n",
    "            # Save the fixed CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Fixed and saved {file}\")\n",
    "        else:\n",
    "            print(f\"Skipping {file}: Not enough columns to modify.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# After this, all CSVs in the folder should have the same structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine all the weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added US1CAAM0003.csv to combined dataset.\n",
      "Added US1CACL0001.csv to combined dataset.\n",
      "Added US1CADN0012.csv to combined dataset.\n",
      "Added US1CAFR0033.csv to combined dataset.\n",
      "Added US1CAHM0029.csv to combined dataset.\n",
      "Added US1CAHM0144.csv to combined dataset.\n",
      "Added US1CALK0018.csv to combined dataset.\n",
      "Added US1CAMD0033.csv to combined dataset.\n",
      "Added US1CAMR0002.csv to combined dataset.\n",
      "Added US1CAMR0011.csv to combined dataset.\n",
      "Added US1CASC0006.csv to combined dataset.\n",
      "Added US1CASD0026.csv to combined dataset.\n",
      "Added US1CASK0016.csv to combined dataset.\n",
      "Added US1CASL0040.csv to combined dataset.\n",
      "Added US1CASU0005.csv to combined dataset.\n",
      "Added US1CASZ0043.csv to combined dataset.\n",
      "Added US1CAVT0017.csv to combined dataset.\n",
      "Added US1CAVT0031.csv to combined dataset.\n",
      "Added USC00040134.csv to combined dataset.\n",
      "Added USC00040161.csv to combined dataset.\n",
      "Added USC00040204.csv to combined dataset.\n",
      "Added USC00040332.csv to combined dataset.\n",
      "Added USC00040543.csv to combined dataset.\n",
      "Added USC00040798.csv to combined dataset.\n",
      "Added USC00041018.csv to combined dataset.\n",
      "Added USC00041075.csv to combined dataset.\n",
      "Added USC00041784.csv to combined dataset.\n",
      "Added USC00041799.csv to combined dataset.\n",
      "Added USC00041805.csv to combined dataset.\n",
      "Added USC00041906.csv to combined dataset.\n",
      "Added USC00042027.csv to combined dataset.\n",
      "Added USC00042269.csv to combined dataset.\n",
      "Added USC00042580.csv to combined dataset.\n",
      "Added USC00044176.csv to combined dataset.\n",
      "Added USC00044518.csv to combined dataset.\n",
      "Added USC00044616.csv to combined dataset.\n",
      "Added USC00045151.csv to combined dataset.\n",
      "Added USC00045212.csv to combined dataset.\n",
      "Added USC00045311.csv to combined dataset.\n",
      "Added USC00045333.csv to combined dataset.\n",
      "Added USC00045352.csv to combined dataset.\n",
      "Added USC00046136.csv to combined dataset.\n",
      "Added USC00046144.csv to combined dataset.\n",
      "Added USC00046172.csv to combined dataset.\n",
      "Added USC00046642.csv to combined dataset.\n",
      "Added USC00046685.csv to combined dataset.\n",
      "Added USC00046754.csv to combined dataset.\n",
      "Added USC00047085.csv to combined dataset.\n",
      "Added USC00047775.csv to combined dataset.\n",
      "Added USC00047950.csv to combined dataset.\n",
      "Added USC00048105.csv to combined dataset.\n",
      "Added USC00048135.csv to combined dataset.\n",
      "Added USC00048338.csv to combined dataset.\n",
      "Added USC00048702.csv to combined dataset.\n",
      "Added USC00049053.csv to combined dataset.\n",
      "Added USC00049083.csv to combined dataset.\n",
      "Added USC00049102.csv to combined dataset.\n",
      "Added USC00049600.csv to combined dataset.\n",
      "Added USC00049855.csv to combined dataset.\n",
      "Added USR0000CALD.csv to combined dataset.\n",
      "Added USR0000CBIH.csv to combined dataset.\n",
      "Added USR0000CCHI.csv to combined dataset.\n",
      "Added USR0000CCRE.csv to combined dataset.\n",
      "Added USR0000CFIG.csv to combined dataset.\n",
      "Added USR0000CIND.csv to combined dataset.\n",
      "Added USR0000CLKL.csv to combined dataset.\n",
      "Added USR0000CMIO.csv to combined dataset.\n",
      "Added USR0000CPIR.csv to combined dataset.\n",
      "Added USR0000CSAC.csv to combined dataset.\n",
      "Added USR0000CSMI.csv to combined dataset.\n",
      "Added USR0000CSNL.csv to combined dataset.\n",
      "Added USR0000CTIM.csv to combined dataset.\n",
      "Added USR0000CWEE.csv to combined dataset.\n",
      "Added USS0019L03S.csv to combined dataset.\n",
      "Added USW00000369.csv to combined dataset.\n",
      "Added USW00023248.csv to combined dataset.\n",
      "Added USW00053139.csv to combined dataset.\n",
      "Added USW00093115.csv to combined dataset.\n",
      "Added USW00093118.csv to combined dataset.\n",
      "Added USW00093205.csv to combined dataset.\n",
      "Added USW00093214.csv to combined dataset.\n",
      "Added USW00093243.csv to combined dataset.\n",
      "All CSV files successfully combined into a single DataFrame.\n",
      "            id      date   obs  obs_value\n",
      "0  US1CAAM0003  20101023  PRCP        102\n",
      "1  US1CAAM0003  20101024  PRCP        546\n",
      "2  US1CAAM0003  20101025  PRCP       1295\n",
      "3  US1CAAM0003  20101026  PRCP          0\n",
      "4  US1CAAM0003  20101027  PRCP          0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder where your CSV files are stored\n",
    "data_folder = \"weathercsv_converted\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Define the expected columns\n",
    "expected_columns = ['id', 'date', 'obs', 'obs_value']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through all CSV files and read them\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Drop unnamed columns (extra columns)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed', na=False)]\n",
    "        \n",
    "        # Ensure it has at least 4 columns\n",
    "        if len(df.columns) >= 4:\n",
    "            df = df.iloc[:, :4]  # Keep only the first four columns\n",
    "            df.columns = expected_columns  # Rename the columns\n",
    "            \n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "            print(f\"Added {file} to combined dataset.\")\n",
    "        else:\n",
    "            print(f\"Skipping {file}: Not enough columns.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(\"All CSV files successfully combined into a single DataFrame.\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame(columns=expected_columns)  # Return an empty DataFrame if no valid files\n",
    "    print(\"No valid CSV files found to combine.\")\n",
    "\n",
    "# Display the DataFrame (optional)\n",
    "print(combined_df.head())  # Show the first few rows\n",
    "\n",
    "# The variable `combined_df` now holds the full dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pivot weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_pivoted= combined_df.pivot(values=\"obs_value\",index=[\"id\",\"date\"],columns=\"obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_data = pd.read_csv(r'fire_data.csv')\n",
    "fire_data = fire_data.dropna()\n",
    "\n",
    "# Drop columns if they exist\n",
    "columns_to_drop = ['CONT_DATE', '_id']\n",
    "fire_data = fire_data.drop(columns=[col for col in columns_to_drop if col in fire_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add corresponding weather station to fire data\n",
    "closest_df= pd.read_csv(r'fire to weather stn conversion.csv')\n",
    "fire_data[\"weatherstn\"] = fire_data[\"Fire station name\"].map(closest_df.set_index(\"fire_name\")[\"closest_weather_id\"])\n",
    "fire_data= fire_data.set_index('weatherstn')\n",
    "\n",
    "allfirestn= fire_data['Fire station name'].unique().tolist()\n",
    "fire_data['Fire station name'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing IDs: ['Colorado River District', 'Siskiyou National Forest', 'Mojave - NPS', 'Fort Yuma Agency', 'Carson City District - BLM', 'Golden Gate National Recreation Area - NPS', 'Orange County', 'CA Desert District - BLM', 'Bakersfield District - BLM (retired code)', 'Beale Air Force Base FD', 'Colorado River Agency', 'Lakeview District', 'Central CA District - BLM', 'Fremont National Forest', 'City of Weed Vol. Fire Dept.', 'Sequoia - Kings Canyon NP', 'San Diego CAL FIRE (retired code)', 'San Diego CAL FIRE', 'Northern CA District - BLM']\n"
     ]
    }
   ],
   "source": [
    "#we have 103 stations, but there is only 83 station in the fire station list\n",
    "#have to redo the list and run all the code again\n",
    "#good news: only have to change the fire station data\n",
    "\n",
    "#first, find out what is missing in the 103, so we can simply add them in \n",
    "\n",
    "missing_stn = list(set(allfirestn) - set(fire_stations[\"fire_name\"]))\n",
    "print(\"Missing IDs:\", missing_stn)\n",
    "\n",
    "#OHH after checking: the missing station does not belong to California\n",
    "#will simpy drop them from the fire dataset\n",
    "\n",
    "\n",
    "# Drop rows where 'id' is in criteria_list\n",
    "df_filtered = fire_data[~fire_data['Fire station name'].isin(missing_stn)]\n",
    "\n",
    "fire_data=df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping data, filling in missing with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather_pivoted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#weather data reshaped: weather_pivoted\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#weather unpivoted (apparently its better to leave data unpivoted for ML?):combined_df\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#fire data with weather station id: fire_data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m weather_dataall\u001b[38;5;241m=\u001b[39m weather_pivoted\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weather_pivoted' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#weather data reshaped: weather_pivoted\n",
    "#weather unpivoted (apparently its better to leave data unpivoted for ML?):combined_df\n",
    "#fire data with weather station id: fire_data\n",
    "\n",
    "weather_dataall= weather_pivoted.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather_dataall' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#try merging with pivoted but separated index weather data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Convert both 'date' columns to datetime with the correct format\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m weather_dataall[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(weather_dataall[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m fire_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALARM_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(fire_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALARM_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m weather_fire_pivotmerge \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(weather_dataall, fire_data, left_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m], right_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweatherstn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALARM_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weather_dataall' is not defined"
     ]
    }
   ],
   "source": [
    "#try merging with pivoted but separated index weather data\n",
    "# Convert both 'date' columns to datetime with the correct format\n",
    "weather_dataall['date'] = pd.to_datetime(weather_dataall['date'], format='%d/%m/%Y')\n",
    "fire_data['ALARM_DATE'] = pd.to_datetime(fire_data['ALARM_DATE'], format='%d/%m/%Y')\n",
    "\n",
    "weather_fire_pivotmerge = pd.merge(weather_dataall, fire_data, left_on=['id', 'date'], right_on=['weatherstn', 'ALARM_DATE'], how='left')\n",
    "# Add a 'fire_occurred' column where 1 indicates fire occurred (fire_area is not NaN) and 0 indicates no fire\n",
    "weather_fire_pivotmerge['fire_occurred'] = weather_fire_pivotmerge['Shape__Area'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try merging with unpivoted index weather data, tall and skinny data\n",
    "unpivot_weather = pd.read_csv(r'combined_df.csv')\n",
    "# Convert both 'date' columns to datetime with the correct format\n",
    "unpivot_weather['date'] = pd.to_datetime(unpivot_weather['date'].astype(str), format='%Y%m%d', errors='coerce')\n",
    "fire_data['ALARM_DATE'] = pd.to_datetime(fire_data['ALARM_DATE'].astype(str), format='%Y%m%d', errors='coerce')\n",
    "\n",
    "unpivotweather_fire_merge = pd.merge(unpivot_weather, fire_data, left_on=['id', 'date'], right_on=['weatherstn', 'ALARM_DATE'], how='left')\n",
    "# Add a 'fire_occurred' column where 1 indicates fire occurred (fire_area is not NaN) and 0 indicates no fire\n",
    "unpivotweather_fire_merge['fire_occurred'] = unpivotweather_fire_merge['Shape__Area'].notna().astype(int)\n",
    "\n",
    "start_date = '1984-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "unpivotweather_fire_merge = unpivotweather_fire_merge[(unpivotweather_fire_merge['date'] >= start_date) & (unpivotweather_fire_merge['date'] <= end_date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill NaN values with 0 for the columns from 'GIS_ACRES' to 'Shape__Length' and update the dataframe\n",
    "#unpivotweather_fire_merge.loc[:, 'GIS_ACRES':'Shape__Length'] = unpivotweather_fire_merge.loc[:, 'GIS_ACRES':'Shape__Length'].fillna(0)\n",
    "\n",
    "\n",
    "# Now the NaN values in the selected columns are filled with 0 in the original dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAYY machine learning time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natal\\anaconda3\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n",
      "c:\\Users\\natal\\anaconda3\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n",
      "c:\\Users\\natal\\anaconda3\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n",
      "c:\\Users\\natal\\anaconda3\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n",
      "c:\\Users\\natal\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:245: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(oob_score=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(oob_score=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regression model: using fire area\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Assuming 'obs' is the categorical column and 'Shape__Area' is your target column\n",
    "encoder = ce.TargetEncoder(cols=['obs'])  # Target encoding the 'obs' column\n",
    "unpivotweather_fire_merge_encoded = encoder.fit_transform(unpivotweather_fire_merge[['obs']], unpivotweather_fire_merge['Shape__Area'])\n",
    "\n",
    "# Now df_encoded will have the target-encoded 'obs' column\n",
    "\n",
    "##remember: when slicing, []for the column index,[] for the .loc\n",
    "# Now unpivotweather_fire_merge_encoded contains the target-encoded 'obs' column\n",
    "# Use 'obs_value' for features and the target 'Shape__Area'\n",
    "X = unpivotweather_fire_merge_encoded.join(unpivotweather_fire_merge['obs_value'])  # Join 'obs_value' with encoded 'obs'\n",
    "Y = unpivotweather_fire_merge['Shape__Area']  # Target variable\n",
    "\n",
    "#training and testing datasets split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting the dataset into training and testing set (80/20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "# Initialize the RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True)\n",
    "\n",
    "#fitting the model to the data\n",
    "# Train the model\n",
    "model.fit(X, Y)\n",
    "\n",
    "# You can now use the regressor to make predictions, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd attempt: i am gonna split the data into 2 version\n",
    "1 version for randomforestregressor (only the fire_area data, taking out dates without fire)\n",
    "1 version for randomforestclassifier (everything, looking at the binary column, yes/no fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regression_ver = unpivotweather_fire_merge.drop([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure 'obs' is treated as categorical\n",
    "unpivotweather_fire_merge['obs'] = unpivotweather_fire_merge['obs'].astype(str)\n",
    "\n",
    "# One-Hot Encoding for 'obs'\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(unpivotweather_fire_merge[['obs']])  \n",
    "\n",
    "# Convert the encoded array into a DataFrame\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(['obs']), index=unpivotweather_fire_merge.index)\n",
    "\n",
    "# Merge the encoded features with 'obs_value'\n",
    "X = X_encoded_df.join(unpivotweather_fire_merge[['obs_value']])\n",
    "\n",
    "# Define the target variable\n",
    "Y = unpivotweather_fire_merge['Shape__Area']\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor with out-of-bag score enabled\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42, oob_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natal\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:245: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 297114181565088.9\n",
      "R-squared: -0.001795301039899888\n",
      "OOB Score: -0.0018949862561954411\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model using only the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"OOB Score: {model.oob_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "[18, 22, 23, 34, 33, ..., 20, 21, 54, 46, 53]\n",
      "Length: 55\n",
      "Categories (55, int64): [0, 1, 2, 3, ..., 51, 52, 53, 54]\n",
      "category\n",
      "  obs  obs_value\n",
      "0  18        102\n",
      "1  18        546\n",
      "2  18       1295\n",
      "3  18          0\n",
      "4  18          0\n",
      "5  18          0\n",
      "6  18          0\n",
      "7  18         36\n",
      "8  18         33\n",
      "9  22          0\n",
      "id                           object\n",
      "date                 datetime64[ns]\n",
      "obs                        category\n",
      "obs_value                     int64\n",
      "ALARM_DATE           datetime64[ns]\n",
      "Fire station name            object\n",
      "UNIT_ID                      object\n",
      "GIS_ACRES                   float64\n",
      "Shape__Area                 float64\n",
      "Shape__Length               float64\n",
      "YEAR_                       float64\n",
      "fire_occurred                 int32\n",
      "dtype: object\n",
      "            obs\n",
      "0  95148.895327\n",
      "1  95148.895327\n",
      "2  95148.895327\n",
      "3  95148.895327\n",
      "4  95148.895327\n"
     ]
    }
   ],
   "source": [
    "#checking if there are different categories\n",
    "print(unpivotweather_fire_merge['obs'].nunique())  # Count unique categories\n",
    "print(unpivotweather_fire_merge['obs'].unique())   # List unique categories\n",
    "#checking 'obs' type\n",
    "unpivotweather_fire_merge['obs'] = unpivotweather_fire_merge['obs'].astype('category')\n",
    "print(unpivotweather_fire_merge['obs'].dtype)  # Should show 'category'\n",
    "print(unpivotweather_fire_merge[['obs', 'obs_value']].head(10))\n",
    "\n",
    "print(unpivotweather_fire_merge.dtypes)\n",
    "unpivotweather_fire_merge['obs_value'] = pd.to_numeric(unpivotweather_fire_merge['obs_value'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 279526900178269.53\n",
      "R-squared: 0.0018132221127208359\n"
     ]
    }
   ],
   "source": [
    "# Predicting the target values of the test set\n",
    "Y_pred = model.predict(X_train)\n",
    "Y_pred\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) and R-squared (R²) score\n",
    "mse = mean_squared_error(Y_train, Y_pred)\n",
    "r2 = r2_score(Y_train, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
