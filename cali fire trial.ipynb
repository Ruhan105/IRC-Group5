{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 – Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "area_fire= pd.read_csv(r'working data\\CA (cleaned+unit id) wildfire area data (1984 to 2023) .csv')\n",
    "#weather_fire= pd.read_csv(r'CA_Weather (cleaned up) 1984-2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the data, dropping extra column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of data, no. of features: (22261, 8)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22261 entries, 0 to 22260\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   _id            11200 non-null  float64\n",
      " 1   ALARM_DATE     10618 non-null  object \n",
      " 2   CONT_DATE      8737 non-null   object \n",
      " 3   UNIT_ID        22195 non-null  object \n",
      " 4   GIS_ACRES      11200 non-null  float64\n",
      " 5   Shape__Area    11200 non-null  float64\n",
      " 6   Shape__Length  11200 non-null  float64\n",
      " 7   YEAR_          11123 non-null  float64\n",
      "dtypes: float64(5), object(3)\n",
      "memory usage: 1.4+ MB\n",
      "data info: None\n"
     ]
    }
   ],
   "source": [
    "area_fire.head()\n",
    "\n",
    "#check the number of features and amount of entries\n",
    "print('no. of data, no. of features:', area_fire.shape)\n",
    "#check the value and type of each column(features)\n",
    "print('data info:', area_fire.info())\n",
    "\n",
    "area_fire= area_fire.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tabula: convert pdf table into pandas df (failed coz it became a weird list, multiple values in one rows?)\n",
    "tried using online pdf table to excel and import as xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "#print(np.where(city['MACS-ID']== unitid)) -- not work due to differnt length\n",
    "\n",
    "# Find values in unitid but not in city['MACS-ID']\n",
    "missing_unitids = list(set(unitid) - set(city['MACS-ID']))\n",
    "\n",
    "print(missing_unitids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'city_mentioned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m agentcity\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magency+unit id.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable 5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#converting the mentioned fire station code to name\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m agent_name\u001b[38;5;241m=\u001b[39m city_mentioned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisplay Value\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m agent_name\n",
      "\u001b[1;31mNameError\u001b[0m: name 'city_mentioned' is not defined"
     ]
    }
   ],
   "source": [
    "#all the station mentioned in area_fire\n",
    "unitid = list(area_fire[\"UNIT_ID\"].unique())\n",
    "\n",
    "agentcity= pd.read_excel(r'agency+unit id.xlsx',sheet_name='Table 5')\n",
    "\n",
    "\n",
    "#converting the mentioned fire station code to name\n",
    "agent_name= city_mentioned['Display Value']\n",
    "agent_name #all the mentioned fire station NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ca fire_agency-list is from \n",
    "https://firescope.caloes.ca.gov/Governance/Group%20Documents/MACS-ID-Agency-List.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8               Camino\n",
       "24             Arcadia\n",
       "54      San Bernardino\n",
       "55      San Bernardino\n",
       "61            Monterey\n",
       "             ...      \n",
       "932            Visalia\n",
       "953            Vallejo\n",
       "955          Camarillo\n",
       "967               Weed\n",
       "1004          Yosemite\n",
       "Name: CITY, Length: 89, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##testing out if the fire station mentioned matches '\"ca fire-Agency-List.xlsx\" since it has city information\n",
    "cafireagencywithcity= pd.read_excel(r'ca fire-Agency-List.xlsx')\n",
    "cafireagencywithcity=cafireagencywithcity.dropna()\n",
    "\n",
    "relevantmasc_id= cafireagencywithcity[cafireagencywithcity['MACS-ID'].isin(unitid)]\n",
    "city_withfire= relevantmasc_id[['MACS-ID', 'CITY']]\n",
    "city_withfire.head()\n",
    "#city_withfire is the conversion between unitid in fire to city name\n",
    "# Create a mapping dictionary (Stored Value → Display Value)\n",
    "city_mapping = dict(zip(city_withfire['MACS-ID'], city_withfire['CITY']))\n",
    "\n",
    "# Map values to create the 'city_name' column in area_fire\n",
    "area_fire[\"city_name\"] = area_fire['UNIT_ID'].map(city_mapping)\n",
    "#dropped missing values\n",
    "area_fire= area_fire.dropna()\n",
    "\n",
    "#the list of city we are gonna use to match with the weather stations\n",
    "citymatchtoweather= relevantmasc_id['CITY']\n",
    "citymatchtoweather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for weather data: we have to align the weather station to the fire stations \n",
    "with their corresponding city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID   ",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "LATITUDE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LONGITUDE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ELEVATION",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "WMO ID",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8766c09b-bcbe-4172-93f2-615feb23735c",
       "rows": [
        [
         "0",
         "USC00040006",
         "39.0333",
         "-122.4333",
         "601.1",
         "CA",
         "ABBOTT MINE                   ",
         "     "
        ],
        [
         "1",
         "USC00040010",
         "38.2178",
         "-121.2014",
         "26.5",
         "CA",
         "ACAMPO 5NE                    ",
         "     "
        ],
        [
         "2",
         "USR0000CACT",
         "34.4458",
         "-118.2",
         "792.5",
         "CA",
         "ACTON CALIFORNIA              ",
         "     "
        ],
        [
         "3",
         "USC00040014",
         "34.4939",
         "-118.2714",
         "863.5",
         "CA",
         "ACTON ESCONDIDO FC261         ",
         "     "
        ],
        [
         "4",
         "USC00040024",
         "34.5833",
         "-117.4167",
         "869.0",
         "CA",
         "ADELANTO                      ",
         "     "
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00040006</td>\n",
       "      <td>39.0333</td>\n",
       "      <td>-122.4333</td>\n",
       "      <td>601.1</td>\n",
       "      <td>CA</td>\n",
       "      <td>ABBOTT MINE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00040010</td>\n",
       "      <td>38.2178</td>\n",
       "      <td>-121.2014</td>\n",
       "      <td>26.5</td>\n",
       "      <td>CA</td>\n",
       "      <td>ACAMPO 5NE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USR0000CACT</td>\n",
       "      <td>34.4458</td>\n",
       "      <td>-118.2000</td>\n",
       "      <td>792.5</td>\n",
       "      <td>CA</td>\n",
       "      <td>ACTON CALIFORNIA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00040014</td>\n",
       "      <td>34.4939</td>\n",
       "      <td>-118.2714</td>\n",
       "      <td>863.5</td>\n",
       "      <td>CA</td>\n",
       "      <td>ACTON ESCONDIDO FC261</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00040024</td>\n",
       "      <td>34.5833</td>\n",
       "      <td>-117.4167</td>\n",
       "      <td>869.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>ADELANTO</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID     LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0  USC00040006   39.0333  -122.4333      601.1    CA   \n",
       "1  USC00040010   38.2178  -121.2014       26.5    CA   \n",
       "2  USR0000CACT   34.4458  -118.2000      792.5    CA   \n",
       "3  USC00040014   34.4939  -118.2714      863.5    CA   \n",
       "4  USC00040024   34.5833  -117.4167      869.0    CA   \n",
       "\n",
       "                             NAME WMO ID  \n",
       "0  ABBOTT MINE                            \n",
       "1  ACAMPO 5NE                             \n",
       "2  ACTON CALIFORNIA                       \n",
       "3  ACTON ESCONDIDO FC261                  \n",
       "4  ADELANTO                               "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allweather_stn = pd.read_excel(r'ghcnd-stations-CA only.xlsx')\n",
    "allweather_stn = allweather_stn.drop(columns=['GSN FLAG', 'HCN/CRN FLAG'])\n",
    "allweather_stn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID   ",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "LATITUDE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LONGITUDE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ELEVATION",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "WMO ID",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8fa46bf8-ef9b-4696-ac8d-b42e15365c48",
       "rows": [
        [
         "39",
         "USC00040161",
         "41.4903",
         "-120.5439",
         "1332.9",
         "CA",
         "ALTURAS                       ",
         "     "
        ],
        [
         "40",
         "USC00040162",
         "41.5147",
         "-120.3642",
         "1617.6",
         "CA",
         "ALTURAS 9 ENE                 ",
         "     "
        ],
        [
         "41",
         "USW00094299",
         "41.4836",
         "-120.5614",
         "1335.9",
         "CA",
         "ALTURAS MUNI AP               ",
         "     "
        ],
        [
         "84",
         "US1CALA0021",
         "34.1617",
         "-118.0285",
         "198.7",
         "CA",
         "ARCADIA 2.1 NNE               ",
         "     "
        ],
        [
         "98",
         "USW00024283",
         "40.9783",
         "-124.1047",
         "64.3",
         "CA",
         "ARCATA EUREKA AP              ",
         "     "
        ],
        [
         "127",
         "USC00040383",
         "38.9072",
         "-121.0839",
         "393.8",
         "CA",
         "AUBURN                        ",
         "     "
        ],
        [
         "128",
         "US1CAPC0055",
         "38.8939",
         "-121.0793",
         "384.4",
         "CA",
         "AUBURN 0.2 NW                 ",
         "     "
        ],
        [
         "129",
         "US1CAPC0054",
         "38.9075",
         "-121.0576",
         "405.1",
         "CA",
         "AUBURN 1.5 NE                 ",
         "     "
        ],
        [
         "130",
         "US1CAPC0012",
         "38.9362",
         "-121.0679",
         "465.7",
         "CA",
         "AUBURN 3.1 N                  ",
         "     "
        ],
        [
         "131",
         "US1CAPC0037",
         "38.9602",
         "-121.0923",
         "440.4",
         "CA",
         "AUBURN 4.8 N                  ",
         "     "
        ],
        [
         "132",
         "US1CAPC0032",
         "38.9629",
         "-121.1153",
         "376.7",
         "CA",
         "AUBURN 5.3 NNW                ",
         "     "
        ],
        [
         "133",
         "USC00040385",
         "38.8833",
         "-121.0667",
         "387.1",
         "CA",
         "AUBURN DAM PROJECT            ",
         "     "
        ],
        [
         "145",
         "USC00040439",
         "35.3833",
         "-119.0167",
         "121.9",
         "CA",
         "BAKERSFIELD                   ",
         "     "
        ],
        [
         "146",
         "USC00040444",
         "35.4186",
         "-119.0508",
         "143.3",
         "CA",
         "BAKERSFIELD 5 NW              ",
         "     "
        ],
        [
         "147",
         "US1CAKN0022",
         "35.3311",
         "-119.1078",
         "112.8",
         "CA",
         "BAKERSFIELD 6.1 WSW           ",
         "     "
        ],
        [
         "148",
         "USW00023155",
         "35.4342",
         "-119.0553",
         "149.4",
         "CA",
         "BAKERSFIELD AP                ",
         "72384"
        ],
        [
         "158",
         "USC00040519",
         "34.9",
         "-117.0333",
         "659.0",
         "CA",
         "BARSTOW                       ",
         "     "
        ],
        [
         "159",
         "USC00040521",
         "34.8928",
         "-117.0219",
         "676.7",
         "CA",
         "BARSTOW                       ",
         "     "
        ],
        [
         "160",
         "US1CASR0023",
         "34.8667",
         "-117.0592",
         "795.2",
         "CA",
         "BARSTOW 0.8 SSE               ",
         "     "
        ],
        [
         "161",
         "US1CASR0025",
         "34.88",
         "-117.118",
         "679.4",
         "CA",
         "BARSTOW 3.0 W                 ",
         "     "
        ],
        [
         "162",
         "US1CASR0035",
         "34.9237",
         "-117.0177",
         "661.4",
         "CA",
         "BARSTOW 4.2 NE                ",
         "     "
        ],
        [
         "163",
         "USW00023161",
         "34.8536",
         "-116.7869",
         "584.9",
         "CA",
         "BARSTOW-DAGGETT AP            ",
         "     "
        ],
        [
         "251",
         "USC00040823",
         "37.3708",
         "-118.3672",
         "1251.5",
         "CA",
         "BISHOP                        ",
         "     "
        ],
        [
         "252",
         "US1CAIN0001",
         "37.3626",
         "-118.3883",
         "1261.9",
         "CA",
         "BISHOP 0.6 SE                 ",
         "     "
        ],
        [
         "253",
         "US1CAIN0013",
         "37.3786",
         "-118.4178",
         "1281.1",
         "CA",
         "BISHOP 1.4 WNW                ",
         "     "
        ],
        [
         "254",
         "US1CAIN0007",
         "37.3854",
         "-118.415",
         "1275.0",
         "CA",
         "BISHOP 1.6 NW                 ",
         "     "
        ],
        [
         "255",
         "US1CAIN0004",
         "37.3878",
         "-118.4141",
         "1274.4",
         "CA",
         "BISHOP 1.7 NW                 ",
         "     "
        ],
        [
         "256",
         "US1CAIN0012",
         "37.3474",
         "-118.4153",
         "1286.6",
         "CA",
         "BISHOP 1.7 SW                 ",
         "     "
        ],
        [
         "257",
         "US1CAIN0010",
         "37.3844",
         "-118.4228",
         "1282.0",
         "CA",
         "BISHOP 1.8 NW                 ",
         "     "
        ],
        [
         "258",
         "US1CAIN0002",
         "37.3227",
         "-118.5404",
         "1829.4",
         "CA",
         "BISHOP 8.5 WSW                ",
         "     "
        ],
        [
         "259",
         "US1CAIN0005",
         "37.4115",
         "-118.5474",
         "1382.9",
         "CA",
         "BISHOP 8.8 WNW                ",
         "     "
        ],
        [
         "260",
         "USW00023157",
         "37.3711",
         "-118.3589",
         "1252.7",
         "CA",
         "BISHOP AP                     ",
         "72480"
        ],
        [
         "261",
         "USC00040819",
         "37.2481",
         "-118.5814",
         "2485.3",
         "CA",
         "BISHOP CREEK INTAKE 2         ",
         "     "
        ],
        [
         "262",
         "USC00040824",
         "37.3667",
         "-118.7167",
         "2863.9",
         "CA",
         "BISHOP UNION CARBIDE          ",
         "     "
        ],
        [
         "388",
         "US1CAVT0019",
         "34.227",
         "-119.035",
         "50.3",
         "CA",
         "CAMARILLO 0.4 NW              ",
         "     "
        ],
        [
         "389",
         "US1CAVT0016",
         "34.2311",
         "-119.0299",
         "54.3",
         "CA",
         "CAMARILLO 0.6 N               ",
         "     "
        ],
        [
         "390",
         "US1CAVT0036",
         "34.2392",
         "-119.0033",
         "75.9",
         "CA",
         "CAMARILLO 1.9 NE              ",
         "     "
        ],
        [
         "391",
         "US1CAVT0012",
         "34.2275",
         "-119.0715",
         "34.7",
         "CA",
         "CAMARILLO 2.4 W               ",
         "     "
        ],
        [
         "392",
         "US1CAVT0035",
         "34.2435",
         "-118.9903",
         "110.9",
         "CA",
         "CAMARILLO 2.7 ENE             ",
         "     "
        ],
        [
         "393",
         "USW00023136",
         "34.2114",
         "-119.0875",
         "21.6",
         "CA",
         "CAMARILLO AP                  ",
         "     "
        ],
        [
         "400",
         "US1CAED0036",
         "38.7547",
         "-120.6712",
         "942.1",
         "CA",
         "CAMINO 1.2 N                  ",
         "     "
        ],
        [
         "401",
         "US1CAED0017",
         "38.7338",
         "-120.711",
         "887.0",
         "CA",
         "CAMINO 2.0 WSW                ",
         "     "
        ],
        [
         "402",
         "US1CAED0012",
         "38.7186",
         "-120.634",
         "947.3",
         "CA",
         "CAMINO 2.6 ESE                ",
         "     "
        ],
        [
         "409",
         "USW00003154",
         "33.3042",
         "-117.355",
         "21.3",
         "CA",
         "CAMP PENDLETON MCAS           ",
         "     "
        ],
        [
         "410",
         "USC00041444",
         "35.3333",
         "-120.6833",
         "189.0",
         "CA",
         "CAMP SAN LUIS OBISPO          ",
         "     "
        ],
        [
         "494",
         "US1CABT0019",
         "39.7525",
         "-121.8028",
         "78.3",
         "CA",
         "CHICO 0.2 ESE                 ",
         "     "
        ],
        [
         "495",
         "US1CABT0023",
         "39.7469",
         "-121.8201",
         "69.5",
         "CA",
         "CHICO 0.8 SW                  ",
         "     "
        ],
        [
         "496",
         "US1CABT0011",
         "39.7387",
         "-121.7984",
         "75.6",
         "CA",
         "CHICO 1.1 SSE                 ",
         "     "
        ],
        [
         "497",
         "US1CABT0007",
         "39.7697",
         "-121.8138",
         "76.5",
         "CA",
         "CHICO 1.2 NNW                 ",
         "     "
        ],
        [
         "498",
         "US1CABT0015",
         "39.7549",
         "-121.7823",
         "110.3",
         "CA",
         "CHICO 1.3 E                   ",
         "     "
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 413
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>USC00040161</td>\n",
       "      <td>41.4903</td>\n",
       "      <td>-120.5439</td>\n",
       "      <td>1332.9</td>\n",
       "      <td>CA</td>\n",
       "      <td>ALTURAS</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>USC00040162</td>\n",
       "      <td>41.5147</td>\n",
       "      <td>-120.3642</td>\n",
       "      <td>1617.6</td>\n",
       "      <td>CA</td>\n",
       "      <td>ALTURAS 9 ENE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>USW00094299</td>\n",
       "      <td>41.4836</td>\n",
       "      <td>-120.5614</td>\n",
       "      <td>1335.9</td>\n",
       "      <td>CA</td>\n",
       "      <td>ALTURAS MUNI AP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>US1CALA0021</td>\n",
       "      <td>34.1617</td>\n",
       "      <td>-118.0285</td>\n",
       "      <td>198.7</td>\n",
       "      <td>CA</td>\n",
       "      <td>ARCADIA 2.1 NNE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>USW00024283</td>\n",
       "      <td>40.9783</td>\n",
       "      <td>-124.1047</td>\n",
       "      <td>64.3</td>\n",
       "      <td>CA</td>\n",
       "      <td>ARCATA EUREKA AP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>USW00053150</td>\n",
       "      <td>37.7592</td>\n",
       "      <td>-119.8208</td>\n",
       "      <td>2017.8</td>\n",
       "      <td>CA</td>\n",
       "      <td>YOSEMITE VILLAGE 12 W</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>USC00049866</td>\n",
       "      <td>41.7025</td>\n",
       "      <td>-122.6417</td>\n",
       "      <td>826.6</td>\n",
       "      <td>CA</td>\n",
       "      <td>YREKA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>US1CASK0005</td>\n",
       "      <td>41.7326</td>\n",
       "      <td>-122.6495</td>\n",
       "      <td>820.5</td>\n",
       "      <td>CA</td>\n",
       "      <td>YREKA 0.9 WNW</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>US1CASK0002</td>\n",
       "      <td>41.6651</td>\n",
       "      <td>-122.6208</td>\n",
       "      <td>895.2</td>\n",
       "      <td>CA</td>\n",
       "      <td>YREKA 4.5 S</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>US1CASK0030</td>\n",
       "      <td>41.6326</td>\n",
       "      <td>-122.5818</td>\n",
       "      <td>897.3</td>\n",
       "      <td>CA</td>\n",
       "      <td>YREKA 7.1 SSE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID     LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "39    USC00040161   41.4903  -120.5439     1332.9    CA   \n",
       "40    USC00040162   41.5147  -120.3642     1617.6    CA   \n",
       "41    USW00094299   41.4836  -120.5614     1335.9    CA   \n",
       "84    US1CALA0021   34.1617  -118.0285      198.7    CA   \n",
       "98    USW00024283   40.9783  -124.1047       64.3    CA   \n",
       "...           ...       ...        ...        ...   ...   \n",
       "3122  USW00053150   37.7592  -119.8208     2017.8    CA   \n",
       "3124  USC00049866   41.7025  -122.6417      826.6    CA   \n",
       "3125  US1CASK0005   41.7326  -122.6495      820.5    CA   \n",
       "3126  US1CASK0002   41.6651  -122.6208      895.2    CA   \n",
       "3127  US1CASK0030   41.6326  -122.5818      897.3    CA   \n",
       "\n",
       "                                NAME WMO ID  \n",
       "39    ALTURAS                                \n",
       "40    ALTURAS 9 ENE                          \n",
       "41    ALTURAS MUNI AP                        \n",
       "84    ARCADIA 2.1 NNE                        \n",
       "98    ARCATA EUREKA AP                       \n",
       "...                              ...    ...  \n",
       "3122  YOSEMITE VILLAGE 12 W                  \n",
       "3124  YREKA                                  \n",
       "3125  YREKA 0.9 WNW                          \n",
       "3126  YREKA 4.5 S                            \n",
       "3127  YREKA 7.1 SSE                          \n",
       "\n",
       "[413 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matching the list of city to the weather stations to get their weather station id\n",
    "\n",
    "weather_stnreq = allweather_stn[allweather_stn['NAME'].apply(lambda x: any(city.lower() in x.lower() for city in citymatchtoweather))]\n",
    "weather_stnreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39      USC00040161\n",
       "40      USC00040162\n",
       "41      USW00094299\n",
       "84      US1CALA0021\n",
       "98      USW00024283\n",
       "           ...     \n",
       "3122    USW00053150\n",
       "3124    USC00049866\n",
       "3125    US1CASK0005\n",
       "3126    US1CASK0002\n",
       "3127    US1CASK0030\n",
       "Name: ID   , Length: 413, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of weather station id we need to get from ftp\n",
    "weatherid_list = weather_stnreq[\"ID   \"]\n",
    "weatherid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "\n",
    "# List of station IDs (modify as needed)\n",
    "\n",
    "\n",
    "# Directory to save downloaded files\n",
    "download_dir = \"weather_data\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "for station_id in weatherid_list:\n",
    "    file_name = f\"{station_id}.csv.gz\"  # NOAA files are in .csv.gz format\n",
    "    file_url = base_url + file_name\n",
    "    local_file_path = os.path.join(download_dir, file_name)\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(file_url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(local_file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {file_name} (Status Code: {response.status_code})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: make a column in area_fire, indicating that these observations are Fire=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to put all 413 weather csv into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read US1CAAL0007.csv.gz with 5014 rows.\n",
      "Saved converted file to: converted_files\\US1CAAL0007.csv\n",
      "Read US1CAAL0022.csv.gz with 1383 rows.\n",
      "Saved converted file to: converted_files\\US1CAAL0022.csv\n",
      "Read US1CAAL0024.csv.gz with 473 rows.\n",
      "Saved converted file to: converted_files\\US1CAAL0024.csv\n",
      "Read US1CAAL0031.csv.gz with 605 rows.\n",
      "Saved converted file to: converted_files\\US1CAAL0031.csv\n",
      "Read US1CAAM0004.csv.gz with 936 rows.\n",
      "Saved converted file to: converted_files\\US1CAAM0004.csv\n",
      "Read US1CAAM0014.csv.gz with 469 rows.\n",
      "Saved converted file to: converted_files\\US1CAAM0014.csv\n",
      "Read US1CAAM0016.csv.gz with 2172 rows.\n",
      "Saved converted file to: converted_files\\US1CAAM0016.csv\n",
      "Read US1CABT0002.csv.gz with 5677 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0002.csv\n",
      "Read US1CABT0003.csv.gz with 198 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0003.csv\n",
      "Read US1CABT0005.csv.gz with 8556 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0005.csv\n",
      "Read US1CABT0007.csv.gz with 2861 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0007.csv\n",
      "Read US1CABT0008.csv.gz with 8230 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0008.csv\n",
      "Read US1CABT0009.csv.gz with 4243 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0009.csv\n",
      "Read US1CABT0010.csv.gz with 379 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0010.csv\n",
      "Read US1CABT0011.csv.gz with 651 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0011.csv\n",
      "Read US1CABT0012.csv.gz with 597 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0012.csv\n",
      "Read US1CABT0013.csv.gz with 1005 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0013.csv\n",
      "Read US1CABT0015.csv.gz with 601 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0015.csv\n",
      "Read US1CABT0017.csv.gz with 3888 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0017.csv\n",
      "Read US1CABT0019.csv.gz with 1399 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0019.csv\n",
      "Read US1CABT0023.csv.gz with 244 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0023.csv\n",
      "Read US1CABT0042.csv.gz with 1679 rows.\n",
      "Saved converted file to: converted_files\\US1CABT0042.csv\n",
      "Read US1CACL0001.csv.gz with 590 rows.\n",
      "Saved converted file to: converted_files\\US1CACL0001.csv\n",
      "Read US1CADN0001.csv.gz with 8329 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0001.csv\n",
      "Read US1CADN0005.csv.gz with 3243 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0005.csv\n",
      "Read US1CADN0008.csv.gz with 4729 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0008.csv\n",
      "Read US1CADN0009.csv.gz with 717 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0009.csv\n",
      "Read US1CADN0011.csv.gz with 4323 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0011.csv\n",
      "Read US1CADN0015.csv.gz with 559 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0015.csv\n",
      "Read US1CADN0016.csv.gz with 301 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0016.csv\n",
      "Read US1CADN0017.csv.gz with 2331 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0017.csv\n",
      "Read US1CADN0021.csv.gz with 1072 rows.\n",
      "Saved converted file to: converted_files\\US1CADN0021.csv\n",
      "Read US1CAED0002.csv.gz with 1864 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0002.csv\n",
      "Read US1CAED0003.csv.gz with 11058 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0003.csv\n",
      "Read US1CAED0010.csv.gz with 10784 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0010.csv\n",
      "Read US1CAED0012.csv.gz with 704 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0012.csv\n",
      "Read US1CAED0017.csv.gz with 7245 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0017.csv\n",
      "Read US1CAED0020.csv.gz with 7422 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0020.csv\n",
      "Read US1CAED0027.csv.gz with 3875 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0027.csv\n",
      "Read US1CAED0035.csv.gz with 502 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0035.csv\n",
      "Read US1CAED0036.csv.gz with 2139 rows.\n",
      "Saved converted file to: converted_files\\US1CAED0036.csv\n",
      "Read US1CAFR0001.csv.gz with 2128 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0001.csv\n",
      "Read US1CAFR0006.csv.gz with 2505 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0006.csv\n",
      "Read US1CAFR0007.csv.gz with 3419 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0007.csv\n",
      "Read US1CAFR0014.csv.gz with 8329 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0014.csv\n",
      "Read US1CAFR0016.csv.gz with 340 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0016.csv\n",
      "Read US1CAFR0020.csv.gz with 521 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0020.csv\n",
      "Read US1CAFR0026.csv.gz with 5227 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0026.csv\n",
      "Read US1CAFR0033.csv.gz with 375 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0033.csv\n",
      "Read US1CAFR0034.csv.gz with 3203 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0034.csv\n",
      "Read US1CAFR0035.csv.gz with 301 rows.\n",
      "Saved converted file to: converted_files\\US1CAFR0035.csv\n",
      "Read US1CAHM0011.csv.gz with 773 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0011.csv\n",
      "Read US1CAHM0012.csv.gz with 5174 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0012.csv\n",
      "Read US1CAHM0018.csv.gz with 248 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0018.csv\n",
      "Read US1CAHM0020.csv.gz with 1207 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0020.csv\n",
      "Read US1CAHM0024.csv.gz with 309 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0024.csv\n",
      "Read US1CAHM0029.csv.gz with 8677 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0029.csv\n",
      "Read US1CAHM0035.csv.gz with 7973 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0035.csv\n",
      "Read US1CAHM0041.csv.gz with 11728 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0041.csv\n",
      "Read US1CAHM0050.csv.gz with 908 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0050.csv\n",
      "Read US1CAHM0068.csv.gz with 315 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0068.csv\n",
      "Read US1CAHM0077.csv.gz with 1390 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0077.csv\n",
      "Read US1CAHM0081.csv.gz with 1415 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0081.csv\n",
      "Read US1CAHM0084.csv.gz with 3054 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0084.csv\n",
      "Read US1CAHM0089.csv.gz with 1193 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0089.csv\n",
      "Read US1CAHM0094.csv.gz with 9089 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0094.csv\n",
      "Read US1CAHM0098.csv.gz with 1846 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0098.csv\n",
      "Read US1CAHM0102.csv.gz with 732 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0102.csv\n",
      "Read US1CAHM0104.csv.gz with 641 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0104.csv\n",
      "Read US1CAHM0113.csv.gz with 1995 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0113.csv\n",
      "Read US1CAHM0114.csv.gz with 268 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0114.csv\n",
      "Read US1CAHM0119.csv.gz with 1084 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0119.csv\n",
      "Read US1CAHM0125.csv.gz with 710 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0125.csv\n",
      "Read US1CAHM0128.csv.gz with 873 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0128.csv\n",
      "Read US1CAHM0138.csv.gz with 1213 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0138.csv\n",
      "Read US1CAHM0148.csv.gz with 698 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0148.csv\n",
      "Read US1CAHM0155.csv.gz with 436 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0155.csv\n",
      "Read US1CAHM0157.csv.gz with 860 rows.\n",
      "Saved converted file to: converted_files\\US1CAHM0157.csv\n",
      "Read US1CAIN0001.csv.gz with 3992 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0001.csv\n",
      "Read US1CAIN0002.csv.gz with 6690 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0002.csv\n",
      "Read US1CAIN0004.csv.gz with 6520 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0004.csv\n",
      "Read US1CAIN0005.csv.gz with 3400 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0005.csv\n",
      "Read US1CAIN0007.csv.gz with 880 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0007.csv\n",
      "Read US1CAIN0010.csv.gz with 1037 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0010.csv\n",
      "Read US1CAIN0012.csv.gz with 5911 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0012.csv\n",
      "Read US1CAIN0013.csv.gz with 5889 rows.\n",
      "Saved converted file to: converted_files\\US1CAIN0013.csv\n",
      "Read US1CAKN0021.csv.gz with 133 rows.\n",
      "Saved converted file to: converted_files\\US1CAKN0021.csv\n",
      "Read US1CAKN0022.csv.gz with 5110 rows.\n",
      "Saved converted file to: converted_files\\US1CAKN0022.csv\n",
      "Read US1CALA0021.csv.gz with 6722 rows.\n",
      "Saved converted file to: converted_files\\US1CALA0021.csv\n",
      "Read US1CALA0030.csv.gz with 309 rows.\n",
      "Saved converted file to: converted_files\\US1CALA0030.csv\n",
      "Read US1CALA0064.csv.gz with 5443 rows.\n",
      "Saved converted file to: converted_files\\US1CALA0064.csv\n",
      "Read US1CALK0013.csv.gz with 348 rows.\n",
      "Saved converted file to: converted_files\\US1CALK0013.csv\n",
      "Read US1CALS0001.csv.gz with 17101 rows.\n",
      "Saved converted file to: converted_files\\US1CALS0001.csv\n",
      "Read US1CALS0002.csv.gz with 5996 rows.\n",
      "Saved converted file to: converted_files\\US1CALS0002.csv\n",
      "Read US1CALS0004.csv.gz with 2539 rows.\n",
      "Saved converted file to: converted_files\\US1CALS0004.csv\n",
      "Read US1CAMA0003.csv.gz with 8657 rows.\n",
      "Saved converted file to: converted_files\\US1CAMA0003.csv\n",
      "Read US1CAMD0012.csv.gz with 2465 rows.\n",
      "Saved converted file to: converted_files\\US1CAMD0012.csv\n",
      "Read US1CAMD0037.csv.gz with 391 rows.\n",
      "Saved converted file to: converted_files\\US1CAMD0037.csv\n",
      "Read US1CAMD0040.csv.gz with 730 rows.\n",
      "Saved converted file to: converted_files\\US1CAMD0040.csv\n",
      "Read US1CAMD0046.csv.gz with 837 rows.\n",
      "Saved converted file to: converted_files\\US1CAMD0046.csv\n",
      "Read US1CAMD0047.csv.gz with 129 rows.\n",
      "Saved converted file to: converted_files\\US1CAMD0047.csv\n",
      "Read US1CAMD0056.csv.gz with 1235 rows.\n",
      "Saved converted file to: converted_files\\US1CAMD0056.csv\n",
      "Read US1CAME0004.csv.gz with 2662 rows.\n",
      "Saved converted file to: converted_files\\US1CAME0004.csv\n",
      "Read US1CAME0006.csv.gz with 189 rows.\n",
      "Saved converted file to: converted_files\\US1CAME0006.csv\n",
      "Read US1CAMN0004.csv.gz with 2530 rows.\n",
      "Saved converted file to: converted_files\\US1CAMN0004.csv\n",
      "Read US1CAMP0001.csv.gz with 11583 rows.\n",
      "Saved converted file to: converted_files\\US1CAMP0001.csv\n",
      "Read US1CAMP0002.csv.gz with 8472 rows.\n",
      "Saved converted file to: converted_files\\US1CAMP0002.csv\n",
      "Read US1CAMP0006.csv.gz with 1596 rows.\n",
      "Saved converted file to: converted_files\\US1CAMP0006.csv\n",
      "Read US1CAMP0008.csv.gz with 1061 rows.\n",
      "Saved converted file to: converted_files\\US1CAMP0008.csv\n",
      "Read US1CAMP0009.csv.gz with 1471 rows.\n",
      "Saved converted file to: converted_files\\US1CAMP0009.csv\n",
      "Read US1CAMR0002.csv.gz with 4535 rows.\n",
      "Saved converted file to: converted_files\\US1CAMR0002.csv\n",
      "Read US1CAMR0009.csv.gz with 10337 rows.\n",
      "Saved converted file to: converted_files\\US1CAMR0009.csv\n",
      "Read US1CAMR0016.csv.gz with 2804 rows.\n",
      "Saved converted file to: converted_files\\US1CAMR0016.csv\n",
      "Read US1CAMT0001.csv.gz with 197 rows.\n",
      "Saved converted file to: converted_files\\US1CAMT0001.csv\n",
      "Read US1CAMT0007.csv.gz with 2753 rows.\n",
      "Saved converted file to: converted_files\\US1CAMT0007.csv\n",
      "Read US1CAMT0029.csv.gz with 479 rows.\n",
      "Saved converted file to: converted_files\\US1CAMT0029.csv\n",
      "Read US1CAMT0030.csv.gz with 995 rows.\n",
      "Saved converted file to: converted_files\\US1CAMT0030.csv\n",
      "Read US1CAMT0049.csv.gz with 133 rows.\n",
      "Saved converted file to: converted_files\\US1CAMT0049.csv\n",
      "Read US1CANV0001.csv.gz with 10668 rows.\n",
      "Saved converted file to: converted_files\\US1CANV0001.csv\n",
      "Read US1CANV0022.csv.gz with 7052 rows.\n",
      "Saved converted file to: converted_files\\US1CANV0022.csv\n",
      "Read US1CANV0032.csv.gz with 320 rows.\n",
      "Saved converted file to: converted_files\\US1CANV0032.csv\n",
      "Read US1CANV0041.csv.gz with 7737 rows.\n",
      "Saved converted file to: converted_files\\US1CANV0041.csv\n",
      "Read US1CANV0053.csv.gz with 2707 rows.\n",
      "Saved converted file to: converted_files\\US1CANV0053.csv\n",
      "Read US1CANV0055.csv.gz with 318 rows.\n",
      "Saved converted file to: converted_files\\US1CANV0055.csv\n",
      "Read US1CANV0057.csv.gz with 1720 rows.\n",
      "Saved converted file to: converted_files\\US1CANV0057.csv\n",
      "Read US1CAOR0025.csv.gz with 1946 rows.\n",
      "Saved converted file to: converted_files\\US1CAOR0025.csv\n",
      "Read US1CAOR0027.csv.gz with 8510 rows.\n",
      "Saved converted file to: converted_files\\US1CAOR0027.csv\n",
      "Read US1CAOR0033.csv.gz with 445 rows.\n",
      "Saved converted file to: converted_files\\US1CAOR0033.csv\n",
      "Read US1CAOR0051.csv.gz with 203 rows.\n",
      "Saved converted file to: converted_files\\US1CAOR0051.csv\n",
      "Read US1CAOR0071.csv.gz with 230 rows.\n",
      "Saved converted file to: converted_files\\US1CAOR0071.csv\n",
      "Read US1CAPC0012.csv.gz with 1924 rows.\n",
      "Saved converted file to: converted_files\\US1CAPC0012.csv\n",
      "Read US1CAPC0032.csv.gz with 963 rows.\n",
      "Saved converted file to: converted_files\\US1CAPC0032.csv\n",
      "Read US1CAPC0037.csv.gz with 3570 rows.\n",
      "Saved converted file to: converted_files\\US1CAPC0037.csv\n",
      "Read US1CAPC0054.csv.gz with 1938 rows.\n",
      "Saved converted file to: converted_files\\US1CAPC0054.csv\n",
      "Read US1CAPC0055.csv.gz with 1509 rows.\n",
      "Saved converted file to: converted_files\\US1CAPC0055.csv\n",
      "Read US1CAPM0006.csv.gz with 1123 rows.\n",
      "Saved converted file to: converted_files\\US1CAPM0006.csv\n",
      "Read US1CARV0017.csv.gz with 5900 rows.\n",
      "Saved converted file to: converted_files\\US1CARV0017.csv\n",
      "Read US1CARV0034.csv.gz with 4949 rows.\n",
      "Saved converted file to: converted_files\\US1CARV0034.csv\n",
      "Read US1CASA0041.csv.gz with 324 rows.\n",
      "Saved converted file to: converted_files\\US1CASA0041.csv\n",
      "Read US1CASB0004.csv.gz with 2059 rows.\n",
      "Saved converted file to: converted_files\\US1CASB0004.csv\n",
      "Read US1CASB0009.csv.gz with 498 rows.\n",
      "Saved converted file to: converted_files\\US1CASB0009.csv\n",
      "Read US1CASB0011.csv.gz with 2251 rows.\n",
      "Saved converted file to: converted_files\\US1CASB0011.csv\n",
      "Read US1CASB0017.csv.gz with 475 rows.\n",
      "Saved converted file to: converted_files\\US1CASB0017.csv\n",
      "Read US1CASC0006.csv.gz with 395 rows.\n",
      "Saved converted file to: converted_files\\US1CASC0006.csv\n",
      "Read US1CASC0045.csv.gz with 8390 rows.\n",
      "Saved converted file to: converted_files\\US1CASC0045.csv\n",
      "Read US1CASC0048.csv.gz with 5285 rows.\n",
      "Saved converted file to: converted_files\\US1CASC0048.csv\n",
      "Read US1CASC0057.csv.gz with 3818 rows.\n",
      "Saved converted file to: converted_files\\US1CASC0057.csv\n",
      "Read US1CASD0006.csv.gz with 10311 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0006.csv\n",
      "Read US1CASD0009.csv.gz with 10898 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0009.csv\n",
      "Read US1CASD0013.csv.gz with 5439 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0013.csv\n",
      "Read US1CASD0017.csv.gz with 271 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0017.csv\n",
      "Read US1CASD0018.csv.gz with 10764 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0018.csv\n",
      "Read US1CASD0020.csv.gz with 482 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0020.csv\n",
      "Read US1CASD0024.csv.gz with 10775 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0024.csv\n",
      "Read US1CASD0035.csv.gz with 10088 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0035.csv\n",
      "Read US1CASD0036.csv.gz with 250 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0036.csv\n",
      "Read US1CASD0040.csv.gz with 9567 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0040.csv\n",
      "Read US1CASD0047.csv.gz with 6296 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0047.csv\n",
      "Read US1CASD0061.csv.gz with 5256 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0061.csv\n",
      "Read US1CASD0064.csv.gz with 2128 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0064.csv\n",
      "Read US1CASD0074.csv.gz with 4576 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0074.csv\n",
      "Read US1CASD0091.csv.gz with 479 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0091.csv\n",
      "Read US1CASD0099.csv.gz with 4930 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0099.csv\n",
      "Read US1CASD0100.csv.gz with 2265 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0100.csv\n",
      "Read US1CASD0122.csv.gz with 265 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0122.csv\n",
      "Read US1CASD0125.csv.gz with 255 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0125.csv\n",
      "Read US1CASD0162.csv.gz with 339 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0162.csv\n",
      "Read US1CASD0180.csv.gz with 1878 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0180.csv\n",
      "Read US1CASD0185.csv.gz with 1924 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0185.csv\n",
      "Read US1CASD0203.csv.gz with 2671 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0203.csv\n",
      "Read US1CASD0209.csv.gz with 290 rows.\n",
      "Saved converted file to: converted_files\\US1CASD0209.csv\n",
      "Read US1CASH0001.csv.gz with 671 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0001.csv\n",
      "Read US1CASH0008.csv.gz with 2165 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0008.csv\n",
      "Read US1CASH0009.csv.gz with 5279 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0009.csv\n",
      "Read US1CASH0010.csv.gz with 10073 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0010.csv\n",
      "Read US1CASH0011.csv.gz with 10198 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0011.csv\n",
      "Read US1CASH0013.csv.gz with 206 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0013.csv\n",
      "Read US1CASH0014.csv.gz with 188 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0014.csv\n",
      "Read US1CASH0018.csv.gz with 9557 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0018.csv\n",
      "Read US1CASH0021.csv.gz with 3755 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0021.csv\n",
      "Read US1CASH0022.csv.gz with 1077 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0022.csv\n",
      "Read US1CASH0024.csv.gz with 4625 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0024.csv\n",
      "Read US1CASH0051.csv.gz with 8261 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0051.csv\n",
      "Read US1CASH0052.csv.gz with 1834 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0052.csv\n",
      "Read US1CASH0056.csv.gz with 7526 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0056.csv\n",
      "Read US1CASH0060.csv.gz with 507 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0060.csv\n",
      "Read US1CASH0061.csv.gz with 2331 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0061.csv\n",
      "Read US1CASH0075.csv.gz with 320 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0075.csv\n",
      "Read US1CASH0077.csv.gz with 3105 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0077.csv\n",
      "Read US1CASH0081.csv.gz with 1090 rows.\n",
      "Saved converted file to: converted_files\\US1CASH0081.csv\n",
      "Read US1CASK0002.csv.gz with 3615 rows.\n",
      "Saved converted file to: converted_files\\US1CASK0002.csv\n",
      "Read US1CASK0003.csv.gz with 11769 rows.\n",
      "Saved converted file to: converted_files\\US1CASK0003.csv\n",
      "Read US1CASK0005.csv.gz with 9205 rows.\n",
      "Saved converted file to: converted_files\\US1CASK0005.csv\n",
      "Read US1CASK0030.csv.gz with 1009 rows.\n",
      "Saved converted file to: converted_files\\US1CASK0030.csv\n",
      "Read US1CASL0018.csv.gz with 1735 rows.\n",
      "Saved converted file to: converted_files\\US1CASL0018.csv\n",
      "Read US1CASN0019.csv.gz with 2354 rows.\n",
      "Saved converted file to: converted_files\\US1CASN0019.csv\n",
      "Read US1CASN0039.csv.gz with 323 rows.\n",
      "Saved converted file to: converted_files\\US1CASN0039.csv\n",
      "Read US1CASN0057.csv.gz with 648 rows.\n",
      "Saved converted file to: converted_files\\US1CASN0057.csv\n",
      "Read US1CASN0062.csv.gz with 7353 rows.\n",
      "Saved converted file to: converted_files\\US1CASN0062.csv\n",
      "Read US1CASN0138.csv.gz with 479 rows.\n",
      "Saved converted file to: converted_files\\US1CASN0138.csv\n",
      "Read US1CASN0174.csv.gz with 1956 rows.\n",
      "Saved converted file to: converted_files\\US1CASN0174.csv\n",
      "Read US1CASO0011.csv.gz with 179 rows.\n",
      "Saved converted file to: converted_files\\US1CASO0011.csv\n",
      "Read US1CASR0017.csv.gz with 1949 rows.\n",
      "Saved converted file to: converted_files\\US1CASR0017.csv\n",
      "Read US1CASR0023.csv.gz with 228 rows.\n",
      "Saved converted file to: converted_files\\US1CASR0023.csv\n",
      "Read US1CASR0025.csv.gz with 555 rows.\n",
      "Saved converted file to: converted_files\\US1CASR0025.csv\n",
      "Read US1CASR0035.csv.gz with 1476 rows.\n",
      "Saved converted file to: converted_files\\US1CASR0035.csv\n",
      "Read US1CASR0055.csv.gz with 4601 rows.\n",
      "Saved converted file to: converted_files\\US1CASR0055.csv\n",
      "Read US1CASR0072.csv.gz with 4626 rows.\n",
      "Saved converted file to: converted_files\\US1CASR0072.csv\n",
      "Read US1CASU0005.csv.gz with 1320 rows.\n",
      "Saved converted file to: converted_files\\US1CASU0005.csv\n",
      "Read US1CASZ0014.csv.gz with 1743 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0014.csv\n",
      "Read US1CASZ0024.csv.gz with 9459 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0024.csv\n",
      "Read US1CASZ0042.csv.gz with 994 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0042.csv\n",
      "Read US1CASZ0044.csv.gz with 4378 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0044.csv\n",
      "Read US1CASZ0050.csv.gz with 512 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0050.csv\n",
      "Read US1CASZ0051.csv.gz with 4573 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0051.csv\n",
      "Read US1CASZ0057.csv.gz with 1830 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0057.csv\n",
      "Read US1CASZ0062.csv.gz with 1192 rows.\n",
      "Saved converted file to: converted_files\\US1CASZ0062.csv\n",
      "Read US1CATH0001.csv.gz with 710 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0001.csv\n",
      "Read US1CATH0005.csv.gz with 7262 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0005.csv\n",
      "Read US1CATH0007.csv.gz with 1396 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0007.csv\n",
      "Read US1CATH0008.csv.gz with 1525 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0008.csv\n",
      "Read US1CATH0009.csv.gz with 2781 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0009.csv\n",
      "Read US1CATH0011.csv.gz with 3778 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0011.csv\n",
      "Read US1CATH0013.csv.gz with 583 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0013.csv\n",
      "Read US1CATH0014.csv.gz with 1448 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0014.csv\n",
      "Read US1CATH0019.csv.gz with 6383 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0019.csv\n",
      "Read US1CATH0022.csv.gz with 1896 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0022.csv\n",
      "Read US1CATH0030.csv.gz with 6755 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0030.csv\n",
      "Read US1CATH0033.csv.gz with 538 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0033.csv\n",
      "Read US1CATH0037.csv.gz with 3906 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0037.csv\n",
      "Read US1CATH0042.csv.gz with 2231 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0042.csv\n",
      "Read US1CATH0043.csv.gz with 351 rows.\n",
      "Saved converted file to: converted_files\\US1CATH0043.csv\n",
      "Read US1CATL0001.csv.gz with 949 rows.\n",
      "Saved converted file to: converted_files\\US1CATL0001.csv\n",
      "Read US1CATL0005.csv.gz with 7061 rows.\n",
      "Saved converted file to: converted_files\\US1CATL0005.csv\n",
      "Read US1CATL0009.csv.gz with 912 rows.\n",
      "Saved converted file to: converted_files\\US1CATL0009.csv\n",
      "Read US1CATL0011.csv.gz with 9136 rows.\n",
      "Saved converted file to: converted_files\\US1CATL0011.csv\n",
      "Read US1CATL0016.csv.gz with 1840 rows.\n",
      "Saved converted file to: converted_files\\US1CATL0016.csv\n",
      "Read US1CATL0024.csv.gz with 1338 rows.\n",
      "Saved converted file to: converted_files\\US1CATL0024.csv\n",
      "Read US1CATM0001.csv.gz with 260 rows.\n",
      "Saved converted file to: converted_files\\US1CATM0001.csv\n",
      "Read US1CATM0007.csv.gz with 1485 rows.\n",
      "Saved converted file to: converted_files\\US1CATM0007.csv\n",
      "Read US1CAVT0001.csv.gz with 10957 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0001.csv\n",
      "Read US1CAVT0005.csv.gz with 2810 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0005.csv\n",
      "Read US1CAVT0009.csv.gz with 2372 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0009.csv\n",
      "Read US1CAVT0010.csv.gz with 474 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0010.csv\n",
      "Read US1CAVT0012.csv.gz with 3270 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0012.csv\n",
      "Read US1CAVT0013.csv.gz with 1724 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0013.csv\n",
      "Read US1CAVT0014.csv.gz with 856 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0014.csv\n",
      "Read US1CAVT0016.csv.gz with 6232 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0016.csv\n",
      "Read US1CAVT0018.csv.gz with 5662 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0018.csv\n",
      "Read US1CAVT0019.csv.gz with 113 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0019.csv\n",
      "Read US1CAVT0026.csv.gz with 222 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0026.csv\n",
      "Read US1CAVT0028.csv.gz with 992 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0028.csv\n",
      "Read US1CAVT0029.csv.gz with 269 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0029.csv\n",
      "Read US1CAVT0033.csv.gz with 224 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0033.csv\n",
      "Read US1CAVT0035.csv.gz with 1157 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0035.csv\n",
      "Read US1CAVT0036.csv.gz with 951 rows.\n",
      "Saved converted file to: converted_files\\US1CAVT0036.csv\n",
      "Read USC00040161.csv.gz with 187525 rows.\n",
      "Saved converted file to: converted_files\\USC00040161.csv\n",
      "Read USC00040162.csv.gz with 3886 rows.\n",
      "Saved converted file to: converted_files\\USC00040162.csv\n",
      "Read USC00040383.csv.gz with 196696 rows.\n",
      "Saved converted file to: converted_files\\USC00040383.csv\n",
      "Read USC00040385.csv.gz with 43508 rows.\n",
      "Saved converted file to: converted_files\\USC00040385.csv\n",
      "Read USC00040439.csv.gz with 48153 rows.\n",
      "Saved converted file to: converted_files\\USC00040439.csv\n",
      "Read USC00040444.csv.gz with 44886 rows.\n",
      "Saved converted file to: converted_files\\USC00040444.csv\n",
      "Read USC00040519.csv.gz with 104329 rows.\n",
      "Saved converted file to: converted_files\\USC00040519.csv\n",
      "Read USC00040521.csv.gz with 95627 rows.\n",
      "Saved converted file to: converted_files\\USC00040521.csv\n",
      "Read USC00040819.csv.gz with 23771 rows.\n",
      "Saved converted file to: converted_files\\USC00040819.csv\n",
      "Read USC00040823.csv.gz with 11463 rows.\n",
      "Saved converted file to: converted_files\\USC00040823.csv\n",
      "Read USC00040824.csv.gz with 22644 rows.\n",
      "Saved converted file to: converted_files\\USC00040824.csv\n",
      "Read USC00041444.csv.gz with 22401 rows.\n",
      "Saved converted file to: converted_files\\USC00041444.csv\n",
      "Read USC00041715.csv.gz with 230712 rows.\n",
      "Saved converted file to: converted_files\\USC00041715.csv\n",
      "Read USC00041758.csv.gz with 245718 rows.\n",
      "Saved converted file to: converted_files\\USC00041758.csv\n",
      "Read USC00042147.csv.gz with 198198 rows.\n",
      "Saved converted file to: converted_files\\USC00042147.csv\n",
      "Read USC00042148.csv.gz with 50596 rows.\n",
      "Saved converted file to: converted_files\\USC00042148.csv\n",
      "Read USC00042150.csv.gz with 1409 rows.\n",
      "Saved converted file to: converted_files\\USC00042150.csv\n",
      "Read USC00042319.csv.gz with 211110 rows.\n",
      "Saved converted file to: converted_files\\USC00042319.csv\n",
      "Read USC00042346.csv.gz with 84301 rows.\n",
      "Saved converted file to: converted_files\\USC00042346.csv\n",
      "Read USC00042705.csv.gz with 86775 rows.\n",
      "Saved converted file to: converted_files\\USC00042705.csv\n",
      "Read USC00042706.csv.gz with 79420 rows.\n",
      "Saved converted file to: converted_files\\USC00042706.csv\n",
      "Read USC00043004.csv.gz with 16811 rows.\n",
      "Saved converted file to: converted_files\\USC00043004.csv\n",
      "Read USC00043050.csv.gz with 19452 rows.\n",
      "Saved converted file to: converted_files\\USC00043050.csv\n",
      "Read USC00043244.csv.gz with 48246 rows.\n",
      "Saved converted file to: converted_files\\USC00043244.csv\n",
      "Read USC00043251.csv.gz with 3254 rows.\n",
      "Saved converted file to: converted_files\\USC00043251.csv\n",
      "Read USC00043256.csv.gz with 42964 rows.\n",
      "Saved converted file to: converted_files\\USC00043256.csv\n",
      "Read USC00044082.csv.gz with 55870 rows.\n",
      "Saved converted file to: converted_files\\USC00044082.csv\n",
      "Read USC00044084.csv.gz with 11638 rows.\n",
      "Saved converted file to: converted_files\\USC00044084.csv\n",
      "Read USC00044089.csv.gz with 6794 rows.\n",
      "Saved converted file to: converted_files\\USC00044089.csv\n",
      "Read USC00044303.csv.gz with 35554 rows.\n",
      "Saved converted file to: converted_files\\USC00044303.csv\n",
      "Read USC00045111.csv.gz with 1432 rows.\n",
      "Saved converted file to: converted_files\\USC00045111.csv\n",
      "Read USC00045112.csv.gz with 5719 rows.\n",
      "Saved converted file to: converted_files\\USC00045112.csv\n",
      "Read USC00045118.csv.gz with 179762 rows.\n",
      "Saved converted file to: converted_files\\USC00045118.csv\n",
      "Read USC00045119.csv.gz with 84887 rows.\n",
      "Saved converted file to: converted_files\\USC00045119.csv\n",
      "Read USC00045120.csv.gz with 140595 rows.\n",
      "Saved converted file to: converted_files\\USC00045120.csv\n",
      "Read USC00045338.csv.gz with 136452 rows.\n",
      "Saved converted file to: converted_files\\USC00045338.csv\n",
      "Read USC00045346.csv.gz with 56928 rows.\n",
      "Saved converted file to: converted_files\\USC00045346.csv\n",
      "Read USC00045352.csv.gz with 17149 rows.\n",
      "Saved converted file to: converted_files\\USC00045352.csv\n",
      "Read USC00045532.csv.gz with 207162 rows.\n",
      "Saved converted file to: converted_files\\USC00045532.csv\n",
      "Read USC00045535.csv.gz with 1422 rows.\n",
      "Saved converted file to: converted_files\\USC00045535.csv\n",
      "Read USC00045541.csv.gz with 11751 rows.\n",
      "Saved converted file to: converted_files\\USC00045541.csv\n",
      "Read USC00045679.csv.gz with 190247 rows.\n",
      "Saved converted file to: converted_files\\USC00045679.csv\n",
      "Read USC00045795.csv.gz with 159080 rows.\n",
      "Saved converted file to: converted_files\\USC00045795.csv\n",
      "Read USC00045802.csv.gz with 49974 rows.\n",
      "Saved converted file to: converted_files\\USC00045802.csv\n",
      "Read USC00045844.csv.gz with 35473 rows.\n",
      "Saved converted file to: converted_files\\USC00045844.csv\n",
      "Read USC00045847.csv.gz with 1274 rows.\n",
      "Saved converted file to: converted_files\\USC00045847.csv\n",
      "Read USC00045853.csv.gz with 4443 rows.\n",
      "Saved converted file to: converted_files\\USC00045853.csv\n",
      "Read USC00046136.csv.gz with 243608 rows.\n",
      "Saved converted file to: converted_files\\USC00046136.csv\n",
      "Read USC00046139.csv.gz with 485 rows.\n",
      "Saved converted file to: converted_files\\USC00046139.csv\n",
      "Read USC00046521.csv.gz with 133000 rows.\n",
      "Saved converted file to: converted_files\\USC00046521.csv\n",
      "Read USC00046522.csv.gz with 66438 rows.\n",
      "Saved converted file to: converted_files\\USC00046522.csv\n",
      "Read USC00046523.csv.gz with 24180 rows.\n",
      "Saved converted file to: converted_files\\USC00046523.csv\n",
      "Read USC00046525.csv.gz with 47340 rows.\n",
      "Saved converted file to: converted_files\\USC00046525.csv\n",
      "Read USC00046527.csv.gz with 6188 rows.\n",
      "Saved converted file to: converted_files\\USC00046527.csv\n",
      "Read USC00046528.csv.gz with 6657 rows.\n",
      "Saved converted file to: converted_files\\USC00046528.csv\n",
      "Read USC00046816.csv.gz with 24565 rows.\n",
      "Saved converted file to: converted_files\\USC00046816.csv\n",
      "Read USC00046818.csv.gz with 12550 rows.\n",
      "Saved converted file to: converted_files\\USC00046818.csv\n",
      "Read USC00046826.csv.gz with 199436 rows.\n",
      "Saved converted file to: converted_files\\USC00046826.csv\n",
      "Read USC00046829.csv.gz with 1371 rows.\n",
      "Saved converted file to: converted_files\\USC00046829.csv\n",
      "Read USC00046960.csv.gz with 165927 rows.\n",
      "Saved converted file to: converted_files\\USC00046960.csv\n",
      "Read USC00046961.csv.gz with 16252 rows.\n",
      "Saved converted file to: converted_files\\USC00046961.csv\n",
      "Read USC00046962.csv.gz with 88122 rows.\n",
      "Saved converted file to: converted_files\\USC00046962.csv\n",
      "Read USC00046963.csv.gz with 1074 rows.\n",
      "Saved converted file to: converted_files\\USC00046963.csv\n",
      "Read USC00047077.csv.gz with 177423 rows.\n",
      "Saved converted file to: converted_files\\USC00047077.csv\n",
      "Read USC00047195.csv.gz with 210828 rows.\n",
      "Saved converted file to: converted_files\\USC00047195.csv\n",
      "Read USC00047197.csv.gz with 1660 rows.\n",
      "Saved converted file to: converted_files\\USC00047197.csv\n",
      "Read USC00047248.csv.gz with 20390 rows.\n",
      "Saved converted file to: converted_files\\USC00047248.csv\n",
      "Read USC00047293.csv.gz with 12269 rows.\n",
      "Saved converted file to: converted_files\\USC00047293.csv\n",
      "Read USC00047294.csv.gz with 1417 rows.\n",
      "Saved converted file to: converted_files\\USC00047294.csv\n",
      "Read USC00047296.csv.gz with 125169 rows.\n",
      "Saved converted file to: converted_files\\USC00047296.csv\n",
      "Read USC00047298.csv.gz with 21070 rows.\n",
      "Saved converted file to: converted_files\\USC00047298.csv\n",
      "Read USC00047300.csv.gz with 16592 rows.\n",
      "Saved converted file to: converted_files\\USC00047300.csv\n",
      "Read USC00047643.csv.gz with 181779 rows.\n",
      "Saved converted file to: converted_files\\USC00047643.csv\n",
      "Read USC00047646.csv.gz with 1429 rows.\n",
      "Saved converted file to: converted_files\\USC00047646.csv\n",
      "Read USC00047649.csv.gz with 1403 rows.\n",
      "Saved converted file to: converted_files\\USC00047649.csv\n",
      "Read USC00047702.csv.gz with 37232 rows.\n",
      "Saved converted file to: converted_files\\USC00047702.csv\n",
      "Read USC00047723.csv.gz with 207331 rows.\n",
      "Saved converted file to: converted_files\\USC00047723.csv\n",
      "Read USC00047851.csv.gz with 180086 rows.\n",
      "Saved converted file to: converted_files\\USC00047851.csv\n",
      "Read USC00047902.csv.gz with 230268 rows.\n",
      "Saved converted file to: converted_files\\USC00047902.csv\n",
      "Read USC00047908.csv.gz with 623 rows.\n",
      "Saved converted file to: converted_files\\USC00047908.csv\n",
      "Read USC00047909.csv.gz with 21487 rows.\n",
      "Saved converted file to: converted_files\\USC00047909.csv\n",
      "Read USC00048099.csv.gz with 34956 rows.\n",
      "Saved converted file to: converted_files\\USC00048099.csv\n",
      "Read USC00048353.csv.gz with 215832 rows.\n",
      "Saved converted file to: converted_files\\USC00048353.csv\n",
      "Read USC00048380.csv.gz with 158841 rows.\n",
      "Saved converted file to: converted_files\\USC00048380.csv\n",
      "Read USC00048701.csv.gz with 12072 rows.\n",
      "Saved converted file to: converted_files\\USC00048701.csv\n",
      "Read USC00048702.csv.gz with 186908 rows.\n",
      "Saved converted file to: converted_files\\USC00048702.csv\n",
      "Read USC00048705.csv.gz with 842 rows.\n",
      "Saved converted file to: converted_files\\USC00048705.csv\n",
      "Read USC00048713.csv.gz with 78928 rows.\n",
      "Saved converted file to: converted_files\\USC00048713.csv\n",
      "Read USC00048904.csv.gz with 6498 rows.\n",
      "Saved converted file to: converted_files\\USC00048904.csv\n",
      "Read USC00048905.csv.gz with 22670 rows.\n",
      "Saved converted file to: converted_files\\USC00048905.csv\n",
      "Read USC00048914.csv.gz with 95738 rows.\n",
      "Saved converted file to: converted_files\\USC00048914.csv\n",
      "Read USC00048917.csv.gz with 96660 rows.\n",
      "Saved converted file to: converted_files\\USC00048917.csv\n",
      "Read USC00049053.csv.gz with 185955 rows.\n",
      "Saved converted file to: converted_files\\USC00049053.csv\n",
      "Read USC00049056.csv.gz with 34107 rows.\n",
      "Saved converted file to: converted_files\\USC00049056.csv\n",
      "Read USC00049087.csv.gz with 172129 rows.\n",
      "Saved converted file to: converted_files\\USC00049087.csv\n",
      "Read USC00049099.csv.gz with 173411 rows.\n",
      "Saved converted file to: converted_files\\USC00049099.csv\n",
      "Read USC00049102.csv.gz with 22629 rows.\n",
      "Saved converted file to: converted_files\\USC00049102.csv\n",
      "Read USC00049219.csv.gz with 29419 rows.\n",
      "Saved converted file to: converted_files\\USC00049219.csv\n",
      "Read USC00049285.csv.gz with 82186 rows.\n",
      "Saved converted file to: converted_files\\USC00049285.csv\n",
      "Read USC00049367.csv.gz with 225280 rows.\n",
      "Saved converted file to: converted_files\\USC00049367.csv\n",
      "Read USC00049468.csv.gz with 1289 rows.\n",
      "Saved converted file to: converted_files\\USC00049468.csv\n",
      "Read USC00049473.csv.gz with 196827 rows.\n",
      "Saved converted file to: converted_files\\USC00049473.csv\n",
      "Read USC00049498.csv.gz with 6135 rows.\n",
      "Saved converted file to: converted_files\\USC00049498.csv\n",
      "Read USC00049499.csv.gz with 68513 rows.\n",
      "Saved converted file to: converted_files\\USC00049499.csv\n",
      "Read USC00049684.csv.gz with 116094 rows.\n",
      "Saved converted file to: converted_files\\USC00049684.csv\n",
      "Read USC00049685.csv.gz with 27282 rows.\n",
      "Saved converted file to: converted_files\\USC00049685.csv\n",
      "Read USC00049699.csv.gz with 213264 rows.\n",
      "Saved converted file to: converted_files\\USC00049699.csv\n",
      "Read USC00049700.csv.gz with 15363 rows.\n",
      "Saved converted file to: converted_files\\USC00049700.csv\n",
      "Read USC00049855.csv.gz with 202995 rows.\n",
      "Saved converted file to: converted_files\\USC00049855.csv\n",
      "Read USC00049866.csv.gz with 217433 rows.\n",
      "Saved converted file to: converted_files\\USC00049866.csv\n",
      "Read USR0000CCHC.csv.gz with 26021 rows.\n",
      "Saved converted file to: converted_files\\USR0000CCHC.csv\n",
      "Read USR0000CFRE.csv.gz with 36440 rows.\n",
      "Saved converted file to: converted_files\\USR0000CFRE.csv\n",
      "Read USR0000CHOO.csv.gz with 30149 rows.\n",
      "Saved converted file to: converted_files\\USR0000CHOO.csv\n",
      "Read USR0000CLAB.csv.gz with 37322 rows.\n",
      "Saved converted file to: converted_files\\USR0000CLAB.csv\n",
      "Read USR0000CMAR.csv.gz with 19421 rows.\n",
      "Saved converted file to: converted_files\\USR0000CMAR.csv\n",
      "Read USR0000CMER.csv.gz with 6638 rows.\n",
      "Saved converted file to: converted_files\\USR0000CMER.csv\n",
      "Read USR0000CMSA.csv.gz with 36596 rows.\n",
      "Saved converted file to: converted_files\\USR0000CMSA.csv\n",
      "Read USR0000CQUI.csv.gz with 38354 rows.\n",
      "Saved converted file to: converted_files\\USR0000CQUI.csv\n",
      "Read USR0000CREA.csv.gz with 25676 rows.\n",
      "Saved converted file to: converted_files\\USR0000CREA.csv\n",
      "Read USR0000CSBB.csv.gz with 26684 rows.\n",
      "Saved converted file to: converted_files\\USR0000CSBB.csv\n",
      "Read USR0000CTHS.csv.gz with 11348 rows.\n",
      "Saved converted file to: converted_files\\USR0000CTHS.csv\n",
      "Read USR0000CWEE.csv.gz with 37700 rows.\n",
      "Saved converted file to: converted_files\\USR0000CWEE.csv\n",
      "Read USR0000CWOO.csv.gz with 19970 rows.\n",
      "Saved converted file to: converted_files\\USR0000CWOO.csv\n",
      "Read USS0019L07S.csv.gz with 108552 rows.\n",
      "Saved converted file to: converted_files\\USS0019L07S.csv\n",
      "Read USW00000369.csv.gz with 15338 rows.\n",
      "Saved converted file to: converted_files\\USW00000369.csv\n",
      "Read USW00003154.csv.gz with 97699 rows.\n",
      "Saved converted file to: converted_files\\USW00003154.csv\n",
      "Read USW00004222.csv.gz with 23716 rows.\n",
      "Saved converted file to: converted_files\\USW00004222.csv\n",
      "Read USW00023136.csv.gz with 147498 rows.\n",
      "Saved converted file to: converted_files\\USW00023136.csv\n",
      "Read USW00023155.csv.gz with 382605 rows.\n",
      "Saved converted file to: converted_files\\USW00023155.csv\n",
      "Read USW00023157.csv.gz with 312362 rows.\n",
      "Saved converted file to: converted_files\\USW00023157.csv\n",
      "Read USW00023161.csv.gz with 251719 rows.\n",
      "Saved converted file to: converted_files\\USW00023161.csv\n",
      "Read USW00023174.csv.gz with 374677 rows.\n",
      "Saved converted file to: converted_files\\USW00023174.csv\n",
      "Read USW00023190.csv.gz with 271637 rows.\n",
      "Saved converted file to: converted_files\\USW00023190.csv\n",
      "Read USW00023196.csv.gz with 18494 rows.\n",
      "Saved converted file to: converted_files\\USW00023196.csv\n",
      "Read USW00023245.csv.gz with 51699 rows.\n",
      "Saved converted file to: converted_files\\USW00023245.csv\n",
      "Read USW00023248.csv.gz with 972 rows.\n",
      "Saved converted file to: converted_files\\USW00023248.csv\n",
      "Read USW00023249.csv.gz with 845 rows.\n",
      "Saved converted file to: converted_files\\USW00023249.csv\n",
      "Read USW00023257.csv.gz with 102252 rows.\n",
      "Saved converted file to: converted_files\\USW00023257.csv\n",
      "Read USW00023259.csv.gz with 117537 rows.\n",
      "Saved converted file to: converted_files\\USW00023259.csv\n",
      "Read USW00023277.csv.gz with 99228 rows.\n",
      "Saved converted file to: converted_files\\USW00023277.csv\n",
      "Read USW00024213.csv.gz with 287293 rows.\n",
      "Saved converted file to: converted_files\\USW00024213.csv\n",
      "Read USW00024214.csv.gz with 2946 rows.\n",
      "Saved converted file to: converted_files\\USW00024214.csv\n",
      "Read USW00024216.csv.gz with 361355 rows.\n",
      "Saved converted file to: converted_files\\USW00024216.csv\n",
      "Read USW00024257.csv.gz with 225089 rows.\n",
      "Saved converted file to: converted_files\\USW00024257.csv\n",
      "Read USW00024283.csv.gz with 111608 rows.\n",
      "Saved converted file to: converted_files\\USW00024283.csv\n",
      "Read USW00024286.csv.gz with 102744 rows.\n",
      "Saved converted file to: converted_files\\USW00024286.csv\n",
      "Read USW00053150.csv.gz with 18874 rows.\n",
      "Saved converted file to: converted_files\\USW00053150.csv\n",
      "Read USW00053152.csv.gz with 17922 rows.\n",
      "Saved converted file to: converted_files\\USW00053152.csv\n",
      "Read USW00093110.csv.gz with 100428 rows.\n",
      "Saved converted file to: converted_files\\USW00093110.csv\n",
      "Read USW00093121.csv.gz with 14487 rows.\n",
      "Saved converted file to: converted_files\\USW00093121.csv\n",
      "Read USW00093124.csv.gz with 1556 rows.\n",
      "Saved converted file to: converted_files\\USW00093124.csv\n",
      "Read USW00093134.csv.gz with 301852 rows.\n",
      "Saved converted file to: converted_files\\USW00093134.csv\n",
      "Read USW00093193.csv.gz with 394271 rows.\n",
      "Saved converted file to: converted_files\\USW00093193.csv\n",
      "Read USW00093206.csv.gz with 99704 rows.\n",
      "Saved converted file to: converted_files\\USW00093206.csv\n",
      "Read USW00093210.csv.gz with 94200 rows.\n",
      "Saved converted file to: converted_files\\USW00093210.csv\n",
      "Read USW00093243.csv.gz with 22382 rows.\n",
      "Saved converted file to: converted_files\\USW00093243.csv\n",
      "Read USW00094299.csv.gz with 94443 rows.\n",
      "Saved converted file to: converted_files\\USW00094299.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path where your datasets are stored\n",
    "data_folder = \"weather_data\"\n",
    "\n",
    "# Define the path where you want to save the converted files\n",
    "converted_folder = \"converted_files\"\n",
    "\n",
    "# Create the converted folder if it doesn't exist\n",
    "os.makedirs(converted_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all .csv.gz files and then convert them into .csv\n",
    "all_files = [f for f in os.listdir(data_folder) if f.endswith('csv.gz')]\n",
    "\n",
    "for file in all_files:\n",
    "    # Open the .gz file in text mode and read using pandas\n",
    "    with gzip.open(f\"{data_folder}/{file}\", 'rt', encoding='utf-8') as f:  # open in text mode with UTF-8 encoding\n",
    "        try:\n",
    "            # Read the CSV file, handle bad lines, and ensure proper separator\n",
    "            df = pd.read_csv(f, low_memory=False, on_bad_lines='skip', sep=',')\n",
    "            \n",
    "            # Print row count for debugging (before saving)\n",
    "            print(f\"Read {file} with {len(df)} rows.\")\n",
    "\n",
    "            # Save the converted .csv file in the new folder (without index)\n",
    "            converted_file_path = os.path.join(converted_folder, f\"{file.split('.')[0]}.csv\")\n",
    "            df.to_csv(converted_file_path, index=False)\n",
    "            \n",
    "            print(f\"Saved converted file to: {converted_file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#test to see if it works\n",
    "testing_csv = pd.read_csv(r'converted_files\\US1CAAL0007.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "with open(r'', 'rb') as file:\n",
    "    content = file.read(50)  # Read the first 20 bytes\n",
    "    print(content)\n",
    "'''\n",
    "import chardet\n",
    "\n",
    "with open(r'converted_files\\US1CAAL0007.csv', 'rb') as f:\n",
    "    raw_data = f.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'converted_files\\US1CAAL0007.csv', encoding='latin1')\n",
    "#it worked, use this encoding after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\US1CAAL0007.csv already has a header.\n",
      "File converted_files\\US1CAAL0022.csv already has a header.\n",
      "File converted_files\\US1CAAL0024.csv already has a header.\n",
      "File converted_files\\US1CAAL0031.csv already has a header.\n",
      "File converted_files\\US1CAAM0004.csv already has a header.\n",
      "File converted_files\\US1CAAM0014.csv already has a header.\n",
      "File converted_files\\US1CAAM0016.csv already has a header.\n",
      "File converted_files\\US1CABT0002.csv already has a header.\n",
      "File converted_files\\US1CABT0003.csv already has a header.\n",
      "File converted_files\\US1CABT0005.csv already has a header.\n",
      "File converted_files\\US1CABT0007.csv already has a header.\n",
      "File converted_files\\US1CABT0008.csv already has a header.\n",
      "File converted_files\\US1CABT0009.csv already has a header.\n",
      "File converted_files\\US1CABT0010.csv already has a header.\n",
      "File converted_files\\US1CABT0011.csv already has a header.\n",
      "File converted_files\\US1CABT0012.csv already has a header.\n",
      "File converted_files\\US1CABT0013.csv already has a header.\n",
      "File converted_files\\US1CABT0015.csv already has a header.\n",
      "File converted_files\\US1CABT0017.csv already has a header.\n",
      "File converted_files\\US1CABT0019.csv already has a header.\n",
      "File converted_files\\US1CABT0023.csv already has a header.\n",
      "File converted_files\\US1CABT0042.csv already has a header.\n",
      "File converted_files\\US1CACL0001.csv already has a header.\n",
      "File converted_files\\US1CADN0001.csv already has a header.\n",
      "File converted_files\\US1CADN0005.csv already has a header.\n",
      "File converted_files\\US1CADN0008.csv already has a header.\n",
      "File converted_files\\US1CADN0009.csv already has a header.\n",
      "File converted_files\\US1CADN0011.csv already has a header.\n",
      "File converted_files\\US1CADN0015.csv already has a header.\n",
      "File converted_files\\US1CADN0016.csv already has a header.\n",
      "File converted_files\\US1CADN0017.csv already has a header.\n",
      "File converted_files\\US1CADN0021.csv already has a header.\n",
      "File converted_files\\US1CAED0002.csv already has a header.\n",
      "File converted_files\\US1CAED0003.csv already has a header.\n",
      "File converted_files\\US1CAED0010.csv already has a header.\n",
      "File converted_files\\US1CAED0012.csv already has a header.\n",
      "File converted_files\\US1CAED0017.csv already has a header.\n",
      "File converted_files\\US1CAED0020.csv already has a header.\n",
      "File converted_files\\US1CAED0027.csv already has a header.\n",
      "File converted_files\\US1CAED0035.csv already has a header.\n",
      "File converted_files\\US1CAED0036.csv already has a header.\n",
      "File converted_files\\US1CAFR0001.csv already has a header.\n",
      "File converted_files\\US1CAFR0006.csv already has a header.\n",
      "File converted_files\\US1CAFR0007.csv already has a header.\n",
      "File converted_files\\US1CAFR0014.csv already has a header.\n",
      "File converted_files\\US1CAFR0016.csv already has a header.\n",
      "File converted_files\\US1CAFR0020.csv already has a header.\n",
      "File converted_files\\US1CAFR0026.csv already has a header.\n",
      "File converted_files\\US1CAFR0033.csv already has a header.\n",
      "File converted_files\\US1CAFR0034.csv already has a header.\n",
      "File converted_files\\US1CAFR0035.csv already has a header.\n",
      "File converted_files\\US1CAHM0011.csv already has a header.\n",
      "File converted_files\\US1CAHM0012.csv already has a header.\n",
      "File converted_files\\US1CAHM0018.csv already has a header.\n",
      "File converted_files\\US1CAHM0020.csv already has a header.\n",
      "File converted_files\\US1CAHM0024.csv already has a header.\n",
      "File converted_files\\US1CAHM0029.csv already has a header.\n",
      "File converted_files\\US1CAHM0035.csv already has a header.\n",
      "File converted_files\\US1CAHM0041.csv already has a header.\n",
      "File converted_files\\US1CAHM0050.csv already has a header.\n",
      "File converted_files\\US1CAHM0068.csv already has a header.\n",
      "File converted_files\\US1CAHM0077.csv already has a header.\n",
      "File converted_files\\US1CAHM0081.csv already has a header.\n",
      "File converted_files\\US1CAHM0084.csv already has a header.\n",
      "File converted_files\\US1CAHM0089.csv already has a header.\n",
      "File converted_files\\US1CAHM0094.csv already has a header.\n",
      "File converted_files\\US1CAHM0098.csv already has a header.\n",
      "File converted_files\\US1CAHM0102.csv already has a header.\n",
      "File converted_files\\US1CAHM0104.csv already has a header.\n",
      "File converted_files\\US1CAHM0113.csv already has a header.\n",
      "File converted_files\\US1CAHM0114.csv already has a header.\n",
      "File converted_files\\US1CAHM0119.csv already has a header.\n",
      "File converted_files\\US1CAHM0125.csv already has a header.\n",
      "File converted_files\\US1CAHM0128.csv already has a header.\n",
      "File converted_files\\US1CAHM0138.csv already has a header.\n",
      "File converted_files\\US1CAHM0148.csv already has a header.\n",
      "File converted_files\\US1CAHM0155.csv already has a header.\n",
      "File converted_files\\US1CAHM0157.csv already has a header.\n",
      "File converted_files\\US1CAIN0001.csv already has a header.\n",
      "File converted_files\\US1CAIN0002.csv already has a header.\n",
      "File converted_files\\US1CAIN0004.csv already has a header.\n",
      "File converted_files\\US1CAIN0005.csv already has a header.\n",
      "File converted_files\\US1CAIN0007.csv already has a header.\n",
      "File converted_files\\US1CAIN0010.csv already has a header.\n",
      "File converted_files\\US1CAIN0012.csv already has a header.\n",
      "File converted_files\\US1CAIN0013.csv already has a header.\n",
      "File converted_files\\US1CAKN0021.csv already has a header.\n",
      "File converted_files\\US1CAKN0022.csv already has a header.\n",
      "File converted_files\\US1CALA0021.csv already has a header.\n",
      "File converted_files\\US1CALA0030.csv already has a header.\n",
      "File converted_files\\US1CALA0064.csv already has a header.\n",
      "File converted_files\\US1CALK0013.csv already has a header.\n",
      "File converted_files\\US1CALS0001.csv already has a header.\n",
      "File converted_files\\US1CALS0002.csv already has a header.\n",
      "File converted_files\\US1CALS0004.csv already has a header.\n",
      "File converted_files\\US1CAMA0003.csv already has a header.\n",
      "File converted_files\\US1CAMD0012.csv already has a header.\n",
      "File converted_files\\US1CAMD0037.csv already has a header.\n",
      "File converted_files\\US1CAMD0040.csv already has a header.\n",
      "File converted_files\\US1CAMD0046.csv already has a header.\n",
      "File converted_files\\US1CAMD0047.csv already has a header.\n",
      "File converted_files\\US1CAMD0056.csv already has a header.\n",
      "File converted_files\\US1CAME0004.csv already has a header.\n",
      "File converted_files\\US1CAME0006.csv already has a header.\n",
      "File converted_files\\US1CAMN0004.csv already has a header.\n",
      "File converted_files\\US1CAMP0001.csv already has a header.\n",
      "File converted_files\\US1CAMP0002.csv already has a header.\n",
      "File converted_files\\US1CAMP0006.csv already has a header.\n",
      "File converted_files\\US1CAMP0008.csv already has a header.\n",
      "File converted_files\\US1CAMP0009.csv already has a header.\n",
      "File converted_files\\US1CAMR0002.csv already has a header.\n",
      "File converted_files\\US1CAMR0009.csv already has a header.\n",
      "File converted_files\\US1CAMR0016.csv already has a header.\n",
      "File converted_files\\US1CAMT0001.csv already has a header.\n",
      "File converted_files\\US1CAMT0007.csv already has a header.\n",
      "File converted_files\\US1CAMT0029.csv already has a header.\n",
      "File converted_files\\US1CAMT0030.csv already has a header.\n",
      "File converted_files\\US1CAMT0049.csv already has a header.\n",
      "File converted_files\\US1CANV0001.csv already has a header.\n",
      "File converted_files\\US1CANV0022.csv already has a header.\n",
      "File converted_files\\US1CANV0032.csv already has a header.\n",
      "File converted_files\\US1CANV0041.csv already has a header.\n",
      "File converted_files\\US1CANV0053.csv already has a header.\n",
      "File converted_files\\US1CANV0055.csv already has a header.\n",
      "File converted_files\\US1CANV0057.csv already has a header.\n",
      "File converted_files\\US1CAOR0025.csv already has a header.\n",
      "File converted_files\\US1CAOR0027.csv already has a header.\n",
      "File converted_files\\US1CAOR0033.csv already has a header.\n",
      "File converted_files\\US1CAOR0051.csv already has a header.\n",
      "File converted_files\\US1CAOR0071.csv already has a header.\n",
      "File converted_files\\US1CAPC0012.csv already has a header.\n",
      "File converted_files\\US1CAPC0032.csv already has a header.\n",
      "File converted_files\\US1CAPC0037.csv already has a header.\n",
      "File converted_files\\US1CAPC0054.csv already has a header.\n",
      "File converted_files\\US1CAPC0055.csv already has a header.\n",
      "File converted_files\\US1CAPM0006.csv already has a header.\n",
      "File converted_files\\US1CARV0017.csv already has a header.\n",
      "File converted_files\\US1CARV0034.csv already has a header.\n",
      "File converted_files\\US1CASA0041.csv already has a header.\n",
      "File converted_files\\US1CASB0004.csv already has a header.\n",
      "File converted_files\\US1CASB0009.csv already has a header.\n",
      "File converted_files\\US1CASB0011.csv already has a header.\n",
      "File converted_files\\US1CASB0017.csv already has a header.\n",
      "File converted_files\\US1CASC0006.csv already has a header.\n",
      "File converted_files\\US1CASC0045.csv already has a header.\n",
      "File converted_files\\US1CASC0048.csv already has a header.\n",
      "File converted_files\\US1CASC0057.csv already has a header.\n",
      "File converted_files\\US1CASD0006.csv already has a header.\n",
      "File converted_files\\US1CASD0009.csv already has a header.\n",
      "File converted_files\\US1CASD0013.csv already has a header.\n",
      "File converted_files\\US1CASD0017.csv already has a header.\n",
      "File converted_files\\US1CASD0018.csv already has a header.\n",
      "File converted_files\\US1CASD0020.csv already has a header.\n",
      "File converted_files\\US1CASD0024.csv already has a header.\n",
      "File converted_files\\US1CASD0035.csv already has a header.\n",
      "File converted_files\\US1CASD0036.csv already has a header.\n",
      "File converted_files\\US1CASD0040.csv already has a header.\n",
      "File converted_files\\US1CASD0047.csv already has a header.\n",
      "File converted_files\\US1CASD0061.csv already has a header.\n",
      "File converted_files\\US1CASD0064.csv already has a header.\n",
      "File converted_files\\US1CASD0074.csv already has a header.\n",
      "File converted_files\\US1CASD0091.csv already has a header.\n",
      "File converted_files\\US1CASD0099.csv already has a header.\n",
      "File converted_files\\US1CASD0100.csv already has a header.\n",
      "File converted_files\\US1CASD0122.csv already has a header.\n",
      "File converted_files\\US1CASD0125.csv already has a header.\n",
      "File converted_files\\US1CASD0162.csv already has a header.\n",
      "File converted_files\\US1CASD0180.csv already has a header.\n",
      "File converted_files\\US1CASD0185.csv already has a header.\n",
      "File converted_files\\US1CASD0203.csv already has a header.\n",
      "File converted_files\\US1CASD0209.csv already has a header.\n",
      "File converted_files\\US1CASH0001.csv already has a header.\n",
      "File converted_files\\US1CASH0008.csv already has a header.\n",
      "File converted_files\\US1CASH0009.csv already has a header.\n",
      "File converted_files\\US1CASH0010.csv already has a header.\n",
      "File converted_files\\US1CASH0011.csv already has a header.\n",
      "File converted_files\\US1CASH0013.csv already has a header.\n",
      "File converted_files\\US1CASH0014.csv already has a header.\n",
      "File converted_files\\US1CASH0018.csv already has a header.\n",
      "File converted_files\\US1CASH0021.csv already has a header.\n",
      "File converted_files\\US1CASH0022.csv already has a header.\n",
      "File converted_files\\US1CASH0024.csv already has a header.\n",
      "File converted_files\\US1CASH0051.csv already has a header.\n",
      "File converted_files\\US1CASH0052.csv already has a header.\n",
      "File converted_files\\US1CASH0056.csv already has a header.\n",
      "File converted_files\\US1CASH0060.csv already has a header.\n",
      "File converted_files\\US1CASH0061.csv already has a header.\n",
      "File converted_files\\US1CASH0075.csv already has a header.\n",
      "File converted_files\\US1CASH0077.csv already has a header.\n",
      "File converted_files\\US1CASH0081.csv already has a header.\n",
      "File converted_files\\US1CASK0002.csv already has a header.\n",
      "File converted_files\\US1CASK0003.csv already has a header.\n",
      "File converted_files\\US1CASK0005.csv already has a header.\n",
      "File converted_files\\US1CASK0030.csv already has a header.\n",
      "File converted_files\\US1CASL0018.csv already has a header.\n",
      "File converted_files\\US1CASN0019.csv already has a header.\n",
      "File converted_files\\US1CASN0039.csv already has a header.\n",
      "File converted_files\\US1CASN0057.csv already has a header.\n",
      "File converted_files\\US1CASN0062.csv already has a header.\n",
      "File converted_files\\US1CASN0138.csv already has a header.\n",
      "File converted_files\\US1CASN0174.csv already has a header.\n",
      "File converted_files\\US1CASO0011.csv already has a header.\n",
      "File converted_files\\US1CASR0017.csv already has a header.\n",
      "File converted_files\\US1CASR0023.csv already has a header.\n",
      "File converted_files\\US1CASR0025.csv already has a header.\n",
      "File converted_files\\US1CASR0035.csv already has a header.\n",
      "File converted_files\\US1CASR0055.csv already has a header.\n",
      "File converted_files\\US1CASR0072.csv already has a header.\n",
      "File converted_files\\US1CASU0005.csv already has a header.\n",
      "File converted_files\\US1CASZ0014.csv already has a header.\n",
      "File converted_files\\US1CASZ0024.csv already has a header.\n",
      "File converted_files\\US1CASZ0042.csv already has a header.\n",
      "File converted_files\\US1CASZ0044.csv already has a header.\n",
      "File converted_files\\US1CASZ0050.csv already has a header.\n",
      "File converted_files\\US1CASZ0051.csv already has a header.\n",
      "File converted_files\\US1CASZ0057.csv already has a header.\n",
      "File converted_files\\US1CASZ0062.csv already has a header.\n",
      "File converted_files\\US1CATH0001.csv already has a header.\n",
      "File converted_files\\US1CATH0005.csv already has a header.\n",
      "File converted_files\\US1CATH0007.csv already has a header.\n",
      "File converted_files\\US1CATH0008.csv already has a header.\n",
      "File converted_files\\US1CATH0009.csv already has a header.\n",
      "File converted_files\\US1CATH0011.csv already has a header.\n",
      "File converted_files\\US1CATH0013.csv already has a header.\n",
      "File converted_files\\US1CATH0014.csv already has a header.\n",
      "File converted_files\\US1CATH0019.csv already has a header.\n",
      "File converted_files\\US1CATH0022.csv already has a header.\n",
      "File converted_files\\US1CATH0030.csv already has a header.\n",
      "File converted_files\\US1CATH0033.csv already has a header.\n",
      "File converted_files\\US1CATH0037.csv already has a header.\n",
      "File converted_files\\US1CATH0042.csv already has a header.\n",
      "File converted_files\\US1CATH0043.csv already has a header.\n",
      "File converted_files\\US1CATL0001.csv already has a header.\n",
      "File converted_files\\US1CATL0005.csv already has a header.\n",
      "File converted_files\\US1CATL0009.csv already has a header.\n",
      "File converted_files\\US1CATL0011.csv already has a header.\n",
      "File converted_files\\US1CATL0016.csv already has a header.\n",
      "File converted_files\\US1CATL0024.csv already has a header.\n",
      "File converted_files\\US1CATM0001.csv already has a header.\n",
      "File converted_files\\US1CATM0007.csv already has a header.\n",
      "File converted_files\\US1CAVT0001.csv already has a header.\n",
      "File converted_files\\US1CAVT0005.csv already has a header.\n",
      "File converted_files\\US1CAVT0009.csv already has a header.\n",
      "File converted_files\\US1CAVT0010.csv already has a header.\n",
      "File converted_files\\US1CAVT0012.csv already has a header.\n",
      "File converted_files\\US1CAVT0013.csv already has a header.\n",
      "File converted_files\\US1CAVT0014.csv already has a header.\n",
      "File converted_files\\US1CAVT0016.csv already has a header.\n",
      "File converted_files\\US1CAVT0018.csv already has a header.\n",
      "File converted_files\\US1CAVT0019.csv already has a header.\n",
      "File converted_files\\US1CAVT0026.csv already has a header.\n",
      "File converted_files\\US1CAVT0028.csv already has a header.\n",
      "File converted_files\\US1CAVT0029.csv already has a header.\n",
      "File converted_files\\US1CAVT0033.csv already has a header.\n",
      "File converted_files\\US1CAVT0035.csv already has a header.\n",
      "File converted_files\\US1CAVT0036.csv already has a header.\n",
      "File converted_files\\USC00040161.csv already has a header.\n",
      "File converted_files\\USC00040162.csv already has a header.\n",
      "File converted_files\\USC00040383.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00040385.csv already has a header.\n",
      "File converted_files\\USC00040439.csv already has a header.\n",
      "File converted_files\\USC00040444.csv already has a header.\n",
      "File converted_files\\USC00040519.csv already has a header.\n",
      "File converted_files\\USC00040521.csv already has a header.\n",
      "File converted_files\\USC00040819.csv already has a header.\n",
      "File converted_files\\USC00040823.csv already has a header.\n",
      "File converted_files\\USC00040824.csv already has a header.\n",
      "File converted_files\\USC00041444.csv already has a header.\n",
      "File converted_files\\USC00041715.csv already has a header.\n",
      "File converted_files\\USC00041758.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00042147.csv already has a header.\n",
      "File converted_files\\USC00042148.csv already has a header.\n",
      "File converted_files\\USC00042150.csv already has a header.\n",
      "File converted_files\\USC00042319.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00042346.csv already has a header.\n",
      "File converted_files\\USC00042705.csv already has a header.\n",
      "File converted_files\\USC00042706.csv already has a header.\n",
      "File converted_files\\USC00043004.csv already has a header.\n",
      "File converted_files\\USC00043050.csv already has a header.\n",
      "File converted_files\\USC00043244.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00043251.csv already has a header.\n",
      "File converted_files\\USC00043256.csv already has a header.\n",
      "File converted_files\\USC00044082.csv already has a header.\n",
      "File converted_files\\USC00044084.csv already has a header.\n",
      "File converted_files\\USC00044089.csv already has a header.\n",
      "File converted_files\\USC00044303.csv already has a header.\n",
      "File converted_files\\USC00045111.csv already has a header.\n",
      "File converted_files\\USC00045112.csv already has a header.\n",
      "File converted_files\\USC00045118.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00045119.csv already has a header.\n",
      "File converted_files\\USC00045120.csv already has a header.\n",
      "File converted_files\\USC00045338.csv already has a header.\n",
      "File converted_files\\USC00045346.csv already has a header.\n",
      "File converted_files\\USC00045352.csv already has a header.\n",
      "File converted_files\\USC00045532.csv already has a header.\n",
      "File converted_files\\USC00045535.csv already has a header.\n",
      "File converted_files\\USC00045541.csv already has a header.\n",
      "File converted_files\\USC00045679.csv already has a header.\n",
      "File converted_files\\USC00045795.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00045802.csv already has a header.\n",
      "File converted_files\\USC00045844.csv already has a header.\n",
      "File converted_files\\USC00045847.csv already has a header.\n",
      "File converted_files\\USC00045853.csv already has a header.\n",
      "File converted_files\\USC00046136.csv already has a header.\n",
      "File converted_files\\USC00046139.csv already has a header.\n",
      "File converted_files\\USC00046521.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00046522.csv already has a header.\n",
      "File converted_files\\USC00046523.csv already has a header.\n",
      "File converted_files\\USC00046525.csv already has a header.\n",
      "File converted_files\\USC00046527.csv already has a header.\n",
      "File converted_files\\USC00046528.csv already has a header.\n",
      "File converted_files\\USC00046816.csv already has a header.\n",
      "File converted_files\\USC00046818.csv already has a header.\n",
      "File converted_files\\USC00046826.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00046829.csv already has a header.\n",
      "File converted_files\\USC00046960.csv already has a header.\n",
      "File converted_files\\USC00046961.csv already has a header.\n",
      "File converted_files\\USC00046962.csv already has a header.\n",
      "File converted_files\\USC00046963.csv already has a header.\n",
      "File converted_files\\USC00047077.csv already has a header.\n",
      "File converted_files\\USC00047195.csv already has a header.\n",
      "File converted_files\\USC00047197.csv already has a header.\n",
      "File converted_files\\USC00047248.csv already has a header.\n",
      "File converted_files\\USC00047293.csv already has a header.\n",
      "File converted_files\\USC00047294.csv already has a header.\n",
      "File converted_files\\USC00047296.csv already has a header.\n",
      "File converted_files\\USC00047298.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00047300.csv already has a header.\n",
      "File converted_files\\USC00047643.csv already has a header.\n",
      "File converted_files\\USC00047646.csv already has a header.\n",
      "File converted_files\\USC00047649.csv already has a header.\n",
      "File converted_files\\USC00047702.csv already has a header.\n",
      "File converted_files\\USC00047723.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00047851.csv already has a header.\n",
      "File converted_files\\USC00047902.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00047908.csv already has a header.\n",
      "File converted_files\\USC00047909.csv already has a header.\n",
      "File converted_files\\USC00048099.csv already has a header.\n",
      "File converted_files\\USC00048353.csv already has a header.\n",
      "File converted_files\\USC00048380.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00048701.csv already has a header.\n",
      "File converted_files\\USC00048702.csv already has a header.\n",
      "File converted_files\\USC00048705.csv already has a header.\n",
      "File converted_files\\USC00048713.csv already has a header.\n",
      "File converted_files\\USC00048904.csv already has a header.\n",
      "File converted_files\\USC00048905.csv already has a header.\n",
      "File converted_files\\USC00048914.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00048917.csv already has a header.\n",
      "File converted_files\\USC00049053.csv already has a header.\n",
      "File converted_files\\USC00049056.csv already has a header.\n",
      "File converted_files\\USC00049087.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00049099.csv already has a header.\n",
      "File converted_files\\USC00049102.csv already has a header.\n",
      "File converted_files\\USC00049219.csv already has a header.\n",
      "File converted_files\\USC00049285.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00049367.csv already has a header.\n",
      "File converted_files\\USC00049468.csv already has a header.\n",
      "File converted_files\\USC00049473.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00049498.csv already has a header.\n",
      "File converted_files\\USC00049499.csv already has a header.\n",
      "File converted_files\\USC00049684.csv already has a header.\n",
      "File converted_files\\USC00049685.csv already has a header.\n",
      "File converted_files\\USC00049699.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USC00049700.csv already has a header.\n",
      "File converted_files\\USC00049855.csv already has a header.\n",
      "File converted_files\\USC00049866.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USR0000CCHC.csv already has a header.\n",
      "File converted_files\\USR0000CFRE.csv already has a header.\n",
      "File converted_files\\USR0000CHOO.csv already has a header.\n",
      "File converted_files\\USR0000CLAB.csv already has a header.\n",
      "File converted_files\\USR0000CMAR.csv already has a header.\n",
      "File converted_files\\USR0000CMER.csv already has a header.\n",
      "File converted_files\\USR0000CMSA.csv already has a header.\n",
      "File converted_files\\USR0000CQUI.csv already has a header.\n",
      "File converted_files\\USR0000CREA.csv already has a header.\n",
      "File converted_files\\USR0000CSBB.csv already has a header.\n",
      "File converted_files\\USR0000CTHS.csv already has a header.\n",
      "File converted_files\\USR0000CWEE.csv already has a header.\n",
      "File converted_files\\USR0000CWOO.csv already has a header.\n",
      "File converted_files\\USS0019L07S.csv already has a header.\n",
      "File converted_files\\USW00000369.csv already has a header.\n",
      "File converted_files\\USW00003154.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USW00004222.csv already has a header.\n",
      "File converted_files\\USW00023136.csv already has a header.\n",
      "File converted_files\\USW00023155.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USW00023157.csv already has a header.\n",
      "File converted_files\\USW00023161.csv already has a header.\n",
      "File converted_files\\USW00023174.csv already has a header.\n",
      "File converted_files\\USW00023190.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USW00023196.csv already has a header.\n",
      "File converted_files\\USW00023245.csv already has a header.\n",
      "File converted_files\\USW00023248.csv already has a header.\n",
      "File converted_files\\USW00023249.csv already has a header.\n",
      "File converted_files\\USW00023257.csv already has a header.\n",
      "File converted_files\\USW00023259.csv already has a header.\n",
      "File converted_files\\USW00023277.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USW00024213.csv already has a header.\n",
      "File converted_files\\USW00024214.csv already has a header.\n",
      "File converted_files\\USW00024216.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USW00024257.csv already has a header.\n",
      "File converted_files\\USW00024283.csv already has a header.\n",
      "File converted_files\\USW00024286.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USW00053150.csv already has a header.\n",
      "File converted_files\\USW00053152.csv already has a header.\n",
      "File converted_files\\USW00093110.csv already has a header.\n",
      "File converted_files\\USW00093121.csv already has a header.\n",
      "File converted_files\\USW00093124.csv already has a header.\n",
      "File converted_files\\USW00093134.csv already has a header.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_24072\\1603579778.py:40: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted_files\\USW00093193.csv already has a header.\n",
      "File converted_files\\USW00093206.csv already has a header.\n",
      "File converted_files\\USW00093210.csv already has a header.\n",
      "File converted_files\\USW00093243.csv already has a header.\n",
      "File converted_files\\USW00094299.csv already has a header.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 112. MiB for an array with shape (1, 14687396) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Concatenate all chunks into a single DataFrame\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(chunks, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Optionally, save the combined DataFrame to a new CSV\u001b[39;00m\n\u001b[0;32m     52\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_output.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     values \u001b[38;5;241m=\u001b[39m _concatenate_join_units(join_units, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    190\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:486\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, copy)\u001b[0m\n\u001b[0;32m    483\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m ensure_block_shape(concat_values, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m concat_compat(to_concat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m!=\u001b[39m empty_dtype_future:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m==\u001b[39m concat_values\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# GH#39122, GH#40893\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natal\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\concat.py:144\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     to_concat_arrs \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[np.ndarray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(to_concat_arrs, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_ea \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kinds \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miuf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;66;03m# GH#39817 cast to object instead of casting bools to numeric\u001b[39;00m\n\u001b[0;32m    148\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 112. MiB for an array with shape (1, 14687396) and data type float64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to the folder where your CSV files are stored\n",
    "folder_path = \"converted_files\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Define the columns you want to use as the header (change as needed)\n",
    "default_columns = ['Source.Name', 'Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7']\n",
    "\n",
    "# Specify the chunk size (e.g., 1 million rows at a time)\n",
    "chunk_size = 1000000\n",
    "\n",
    "# Initialize an empty list to store individual DataFrames\n",
    "chunks = []\n",
    "\n",
    "# Loop through all CSV files and add headers if missing, then read in chunks\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        # Read the first few rows to check if the file has a header\n",
    "        df_preview = pd.read_csv(file, encoding='ISO-8859-1', nrows=5, header=None)\n",
    "\n",
    "        # Check if the first row contains invalid values that suggest it's not a header\n",
    "        if df_preview.iloc[0].isna().all() or df_preview.iloc[0].apply(lambda x: isinstance(x, (int, float))).all():\n",
    "            print(f\"Adding header to {file}\")\n",
    "            \n",
    "            # If no valid header, we will write the default header to the CSV\n",
    "            with open(file, 'r+', encoding='ISO-8859-1') as f:\n",
    "                content = f.readlines()\n",
    "                content[0] = \",\".join(default_columns) + '\\n'  # Modify the first line to add header\n",
    "                f.seek(0)\n",
    "                f.writelines(content)  # Rewrite the file with the header added\n",
    "        else:\n",
    "            print(f\"File {file} already has a header.\")\n",
    "        \n",
    "        # Read the CSV file in chunks after adding header (or confirming it's already there)\n",
    "        for chunk in pd.read_csv(file, encoding='ISO-8859-1', chunksize=chunk_size, on_bad_lines='skip'):\n",
    "            # Drop columns 5-8 (indexes 4-7)\n",
    "            chunk = chunk.drop(chunk.columns[4:8], axis=1)\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "combined_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the combined DataFrame to a new CSV\n",
    "combined_df.to_csv(\"combined_output.csv\", index=False)\n",
    "\n",
    "# Print the first few rows to confirm\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is done on the original area_fire, if you load it again, these dropping code would be needed to clean up area_fire\n",
    "#area_fire=area_fire.drop(['CONT_DATE'],axis=1)\n",
    "#area_fire=area_fire.drop(['_id'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Join and drop unmatched values\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mjoin(scores, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 'inner' keeps only matching rows\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Join and drop unmatched values\n",
    "df = df.join(scores, how='inner')  # 'inner' keeps only matching rows\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRE_START_DAY</th>\n",
       "      <th>PRECIPITATION</th>\n",
       "      <th>MAX_TEMP</th>\n",
       "      <th>MIN_TEMP</th>\n",
       "      <th>AVG_WIND_SPEED</th>\n",
       "      <th>TEMP_RANGE</th>\n",
       "      <th>WIND_TEMP_RATIO</th>\n",
       "      <th>LAGGED_PRECIPITATION</th>\n",
       "      <th>LAGGED_AVG_WIND_SPEED</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>DAY_OF_YEAR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31/12/2023</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "      <td>4.92</td>\n",
       "      <td>9</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.558571</td>\n",
       "      <td>Winter</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30/12/2023</th>\n",
       "      <td>False</td>\n",
       "      <td>0.62</td>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.558571</td>\n",
       "      <td>Winter</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/12/2023</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>65</td>\n",
       "      <td>52</td>\n",
       "      <td>3.80</td>\n",
       "      <td>13</td>\n",
       "      <td>0.058462</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.175714</td>\n",
       "      <td>Winter</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/12/2023</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>5.59</td>\n",
       "      <td>14</td>\n",
       "      <td>0.087344</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.782857</td>\n",
       "      <td>Winter</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/12/2023</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62</td>\n",
       "      <td>52</td>\n",
       "      <td>5.59</td>\n",
       "      <td>10</td>\n",
       "      <td>0.090161</td>\n",
       "      <td>1.83</td>\n",
       "      <td>6.198571</td>\n",
       "      <td>Winter</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/1/1984</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74</td>\n",
       "      <td>49</td>\n",
       "      <td>5.14</td>\n",
       "      <td>25</td>\n",
       "      <td>0.069459</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>Winter</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/1/1984</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>76</td>\n",
       "      <td>45</td>\n",
       "      <td>4.70</td>\n",
       "      <td>31</td>\n",
       "      <td>0.061842</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.090000</td>\n",
       "      <td>Winter</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/1/1984</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70</td>\n",
       "      <td>47</td>\n",
       "      <td>5.37</td>\n",
       "      <td>23</td>\n",
       "      <td>0.076714</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.220000</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/1/1984</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71</td>\n",
       "      <td>46</td>\n",
       "      <td>5.59</td>\n",
       "      <td>25</td>\n",
       "      <td>0.078732</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.145000</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/1984</th>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79</td>\n",
       "      <td>51</td>\n",
       "      <td>4.70</td>\n",
       "      <td>28</td>\n",
       "      <td>0.059494</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>Winter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14610 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FIRE_START_DAY  PRECIPITATION  MAX_TEMP  MIN_TEMP  AVG_WIND_SPEED  \\\n",
       "DATE                                                                            \n",
       "31/12/2023           False           0.00        60        51            4.92   \n",
       "30/12/2023           False           0.62        62        54            8.50   \n",
       "29/12/2023           False           0.00        65        52            3.80   \n",
       "28/12/2023           False           0.00        64        50            5.59   \n",
       "27/12/2023           False           0.00        62        52            5.59   \n",
       "...                    ...            ...       ...       ...             ...   \n",
       "5/1/1984             False           0.00        74        49            5.14   \n",
       "4/1/1984             False           0.00        76        45            4.70   \n",
       "3/1/1984             False           0.00        70        47            5.37   \n",
       "2/1/1984             False           0.00        71        46            5.59   \n",
       "1/1/1984             False           0.00        79        51            4.70   \n",
       "\n",
       "            TEMP_RANGE  WIND_TEMP_RATIO  LAGGED_PRECIPITATION  \\\n",
       "DATE                                                            \n",
       "31/12/2023           9         0.082000                  0.62   \n",
       "30/12/2023           8         0.137097                  0.62   \n",
       "29/12/2023          13         0.058462                  0.00   \n",
       "28/12/2023          14         0.087344                  0.50   \n",
       "27/12/2023          10         0.090161                  1.83   \n",
       "...                ...              ...                   ...   \n",
       "5/1/1984            25         0.069459                  0.00   \n",
       "4/1/1984            31         0.061842                  0.00   \n",
       "3/1/1984            23         0.076714                  0.00   \n",
       "2/1/1984            25         0.078732                  0.00   \n",
       "1/1/1984            28         0.059494                  0.00   \n",
       "\n",
       "            LAGGED_AVG_WIND_SPEED  SEASON  DAY_OF_YEAR  \n",
       "DATE                                                    \n",
       "31/12/2023               5.558571  Winter          365  \n",
       "30/12/2023               5.558571  Winter          364  \n",
       "29/12/2023               5.175714  Winter          363  \n",
       "28/12/2023               5.782857  Winter          362  \n",
       "27/12/2023               6.198571  Winter          361  \n",
       "...                           ...     ...          ...  \n",
       "5/1/1984                 5.100000  Winter            5  \n",
       "4/1/1984                 5.090000  Winter            4  \n",
       "3/1/1984                 5.220000  Winter            3  \n",
       "2/1/1984                 5.145000  Winter            2  \n",
       "1/1/1984                 4.700000  Winter            1  \n",
       "\n",
       "[14610 rows x 11 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_date=area_fire.iloc[:, 0:4]\n",
    "area_date.set_index(['ALARM_DATE'])\n",
    "weather_fire.set_index(['DATE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 – Preprocess the Dataset\n",
    " \n",
    " The features are all on different scales. To bring them all to a common scale, we’ll use the **StandardScaler** that transforms the features to have zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thoughts\n",
    "run a PCA to see which top 2-3 factors are the most crucial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ca_fire' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal width\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal width\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m std_scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 6\u001b[0m scaled_cafire \u001b[38;5;241m=\u001b[39m std_scaler\u001b[38;5;241m.\u001b[39mfit_transform(ca_fire)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ca_fire' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#separate features and\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "std_scaler = StandardScaler()\n",
    "scaled_cafire = std_scaler.fit_transform(ca_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
